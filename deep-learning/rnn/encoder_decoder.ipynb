{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/encoder_decoder.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "87540b40e3958a06"
   },
   "id": "87540b40e3958a06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder-Decoder architecture\n",
    "\n",
    "The [Encoder-Decoder architecture](https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392) is a neural network architecture used in sequence-to-sequence (Seq2Seq) tasks. It is composed of two main parts: the encoder and the decoder. The encoder processes the input sequence and compresses it into a fixed-size internal representation (hidden state of context vector). The decoder is a conditional language model that generates the output sequence.\n",
    "\n",
    "On of the common usages of the Encoder-Decoder architecture is in [neural machine translation](https://en.wikipedia.org/wiki/Neural_machine_translation), where the input sequence is a sentence in one language and the output sequence is the translation of the sentence in another language. The encoder processes the input sentence and compresses it into a fixed-size internal representation. The decoder generates the translation of the sentence in the target language.\n",
    "\n",
    "In this notebook, we will implement a simple Encoder-Decoder architecture using Recurrent Neural Networks (RNN) to translate English into Spanish. The Enoder is a simple bidirectional LSTM network, and the decoder is a simple LSTM network. \n",
    "\n",
    "<img src=\"img/encoder-decoder.jpg\" width=\"1200\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af819f8c13c3ccfa"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f099c66705669f8",
   "metadata": {
    "id": "5f099c66705669f8",
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:13.697818Z",
     "start_time": "2024-12-03T16:30:48.784245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/rnn'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone --depth 1 https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "    !cp {directory}/img/* img/.\n",
    "\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important variables\n",
    "\n",
    "We define the following variables:\n",
    "- `vocab_size`: the size of the vocabulary (number of unique words in both English and Spanish languages).\n",
    "- `max_length`: the maximum length of the input and output sequences (in words). If a sequence is longer than this, it will be truncated. If it is shorter, it will be padded.\n",
    "- `chars_to_remove`: a list of characters to remove from the text.\n",
    "- `train_size_percentage`: the percentage of the data to use for training ([0-100]).\n",
    "- `embedding_size`: the size of the embedding layer (hyperparameter).\n",
    "- `n_epochs`: the maximum number of epochs to train the model (early stopping is used).\n",
    "- `SOS_word`, `EOS_word`: the start and end of sentence special words.\n",
    "- `n_lstm_units`: the number of LSTM units in the Encoder and Decoder RNNs.\n",
    "- `model_file_name`: the file name to save or load the trained model. If the file exists, the model is loaded from disk, otherwise, the model is trained and saved."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65cd51ca9dd147d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab_size = 1_000\n",
    "max_length = 50\n",
    "chars_to_remove = [\"¡\", \"¿\"]\n",
    "train_size_percentage = 85\n",
    "embedding_size = 128\n",
    "n_epochs = 10\n",
    "n_lstm_units = 512\n",
    "SOS_word, EOS_word = \"startofsentence\", \"endofsentence\"\n",
    "model_file_name = 'data/english_spanish_encoder_decoder.keras'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:13.706363Z",
     "start_time": "2024-12-03T16:31:13.701181Z"
    }
   },
   "id": "958e5ce1e4e8001c",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data\n",
    "\n",
    "Our dataset is a collection of 118,964 English-Spanish sentence pairs taken from [here](https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip). We load the file and remove special characters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e6ac944f5fef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read the contents of the data/english-spanish.txt file\n",
    "with open(\"data/english-spanish.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# remove the special characters\n",
    "for special_char in chars_to_remove:\n",
    "    text = text.replace(special_char, \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:13.777102Z",
     "start_time": "2024-12-03T16:31:13.708599Z"
    }
   },
   "id": "e14a73f497875d41",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get a list of the English and Spanish sentences by splitting each line by the tab character. We shuffle the list of pairs, convert it to a pair of lists and show some examples in them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b5e3f3027b4b20"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 118,964.\n",
      "Some example translations:\n",
      "\t1: Let me show you an example. -> Déjame enseñarte un ejemplo.\n",
      "\t2: It's a town of 3,000 people. -> Es un pueblo de tres mil almas.\n",
      "\t3: They live two flights up. -> Viven dos pisos más arriba.\n",
      "\t4: Won't you come with me? -> No vienes conmigo?\n",
      "\t5: No one knows whether he loves her or not. -> Nadie sabe si él la quiere o no.\n"
     ]
    }
   ],
   "source": [
    "# take the English and Spanish sentences, by splitting each line by the tab character\n",
    "pairs: list[(str, str)] = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "\n",
    "# take a list of pairs and returns a pair of lists: one with the English sentences and one with the Spanish sentences\n",
    "sentences_en, sentences_es = zip(*pairs)\n",
    "\n",
    "assert (n_sentences := len(sentences_en)) == len(sentences_es)\n",
    "print(f\"Number of sentences: {n_sentences:,}.\")\n",
    "\n",
    "print(\"Some example translations:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\t{i+1}: {sentences_en[i]} -> {sentences_es[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:14.272220Z",
     "start_time": "2024-12-03T16:31:13.779573Z"
    }
   },
   "id": "6ed0b0a854205a7d",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Encoder-Decoder ANN has two inputs: one for the Encoder (English) and another one for the Decoder (Spanish). Both are strings. However, we create a `TextVectorization` layer for each input, which transforms a batch of strings into a list of token indices or ids (ints). Upon creation, we pass the vocabulary size and the maximum length of the sequences.  \n",
    "\n",
    "Word index/id definition is performed with the `adapt` method, which transforms each input sentence into a list of word indices, considering the vocabulary size. The most frequent words will be mapped to the first token indices, and the least frequent words to the last token indices. Those with a frequency below the vocabulary size will be mapped to the same token index [UNK].\n",
    "\n",
    "For the Decoder input (Spanish sentences), we include the start and end of sentence tokens (SOS and EOS). SOS will indicate the Decoder to start generating the first Spanish word, and EOS will indicate the end of the sentence (termination of generation)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0c8581c7e4dd658"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "Some example English words: ['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "Some example Spanish words: ['', '[UNK]', 'startofsentence', 'endofsentence', 'de', 'que', 'a', 'no', 'tom', 'la']\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization is as keras layer that converts a batch of strings into either a list of token indices / ids (ints)\n",
    "# It could also output a dense representation of the strings, where each token is represented by a dense vector (not used here)\n",
    "# sentences longer than `max_length` are truncated, and shorter sentences are padded with zeros\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "\n",
    "# adapt makes the layer to transform each input sentence into a list of word indices, considering the vocabulary size\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "# we adapt the Spanish  layer to the Spanish sentences, including the start and end of sentence tokens\n",
    "text_vec_layer_es.adapt([f\"{SOS_word} {sentence} {EOS_word}\" for sentence in sentences_es])\n",
    "\n",
    "print(f\"Some example English words: {text_vec_layer_en.get_vocabulary()[:10]}\")  # 0 is padding, visualized as ''\n",
    "print(f\"Some example Spanish words: {text_vec_layer_es.get_vocabulary()[:10]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:28.385668Z",
     "start_time": "2024-12-03T16:31:14.276554Z"
    }
   },
   "id": "da62a9a2f125d58",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the data into training and validation sets (both for Encoder and Decoder inputs). We include the start of sentence token at the beginning of each sentence. The output of the Decoder is the same as the input, but shifted one position to the right, and with the end of sentence token at the end of each sentence.\n",
    "\n",
    "The input is one full English sentence (e.g., \"I like soccer\") for the Encoder and the corresponding Spanish sentence prefixed with SOS (e.g., \"SOS Me gusta el fútbol\") for the Decoder. The output is the Spanish sentence without SOS and with EOS at the end (e.g., \"Me gusta el fútbol EOS\"). In this way, give \"I like soccer\" to the Encoder, and SOS to the Decoder, the latter will generate \"Me\". This output will be passed again to the Decoder concatenated to the previous input (i.e., \"SOS Me\"), which will generate \"gusta\", and so on, until it generates the EOS."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9eda0d14cb02e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# we split the data into training and validation sets\n",
    "# we first take the input for the Encoder (English sentences)\n",
    "X_train_encoder = tf.constant(sentences_en[:n_sentences * train_size_percentage // 100])\n",
    "X_valid_encoder = tf.constant(sentences_en[n_sentences*train_size_percentage//100:])\n",
    "\n",
    "# then, we take the input for the Decoder (Spanish sentences)\n",
    "# We include the SOS at the beginning of each sentence. This is because we want the Decoder to start generating\n",
    "# the first Spanish word, by passing SOS as the first input. Then, the Decoder will generate the first word and\n",
    "# we will pass it to the Decoder again, so it can generate the second word, and so on, until it generates the EOS.\n",
    "# EOS does not need to be added to the input, since we want the Decoder to generate it (it will be added to\n",
    "# Y training dataset).\n",
    "X_train_decoder = tf.constant([f\"{SOS_word} {sentence}\" for sentence in\n",
    "                               sentences_es[:n_sentences * train_size_percentage // 100]])\n",
    "X_valid_decoder = tf.constant([f\"{SOS_word} {sentence}\" for sentence in\n",
    "                               sentences_es[n_sentences * train_size_percentage // 100:]])\n",
    "\n",
    "# The output of the Decoder is the same as the input, but shifted one position to the right, and with EOS at the end\n",
    "# of each sentence. This is because we want the Decoder to generate the first word of the Spanish sentence, then\n",
    "# the second word, and so on, until it generates the EOS.\n",
    "Y_train = text_vec_layer_es([f\"{sentence} {EOS_word}\" for sentence in sentences_es[:n_sentences * train_size_percentage // 100]])\n",
    "Y_valid = text_vec_layer_es([f\"{sentence} {EOS_word}\" for sentence in sentences_es[n_sentences * train_size_percentage // 100:]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:29.380861Z",
     "start_time": "2024-12-03T16:31:28.388459Z"
    }
   },
   "id": "fe9335f6522bdf48",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the model\n",
    "\n",
    "Let's create the Encoder-Decoder ANN with two recurrent neural networks (RNNs). The Encoder is a bidirectional LSTM network, and the Decoder is a simple LSTM network. The Encoder processes the input sequence and compresses it into a fixed-size internal representation. The Decoder generates the output sequence. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25d3b8c50f945417"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_model(n_lstm_units_p: int, vocab_size_p: int) -> Model:\n",
    "    \"\"\"\n",
    "    Creates a Keras model for the Encoder-Decoder architecture.\n",
    "    :param n_lstm_units_p: Number of LSTM units in the Encoder and Decoder.\n",
    "    :param vocab_size_p: Vocabulary size.\n",
    "    :return: The model\n",
    "    \"\"\"\n",
    "    # Both the Encoder and the Decoder will receive a batch of sentences (strings) as input \n",
    "    # (English sentences for the Encoder, and Spanish sentences for the Decoder).\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "    # We connect the inputs t the text vectorization layers using the Keras functional API\n",
    "    # In this way, the input sequences are converted into lists of word indices / ids using the TextVectorization layers\n",
    "    encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "    decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "    # The word indices are then converted into dense vectors using an Embedding layer of `embedding_size` dimensions\n",
    "    # The padding character zero is masked out, so it is ignored by the model (its weight is not updated/learned). This speeds up training.\n",
    "    encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size_p, embedding_size, mask_zero=True)\n",
    "    decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size_p, embedding_size, mask_zero=True)\n",
    "    # we connect the embedding layers to the input indices (functional API)\n",
    "    encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "    decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "    # We create the Encoder as a single bidirectional LSTM layer with half of the units (two LSTMs, one for each direction)\n",
    "    # Return_state=True => gets a reference to the layer’s final state (not the output for all the RNN steps)\n",
    "    # Since we are using a bi-LSTM layer, the final state is a tuple containing 2 short- and 2 long-term states,\n",
    "    # one pair for each direction (that is why we use *encoder_states, to store the four states in one single tuple variable)\n",
    "    encoder = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(n_lstm_units_p // 2, return_state=True))\n",
    "    encoder_outputs, *encoder_states = encoder(encoder_embeddings)\n",
    "\n",
    "    # we concatenate the states of the left and right LSTMs (first, the 2 short-term states and then the 2 long-term states)\n",
    "    # this way, we get a single state for each type (short and long-term) to be passed \n",
    "    # to the one-directional Decoder RNN as its initial state (conditional language model)\n",
    "    encoder_state = [tf.concat([encoder_states[0], encoder_states[2]], axis=-1),  # short-term (0 & 2)\n",
    "                     tf.concat([encoder_states[1], encoder_states[3]], axis=-1)]  # long-term (1 & 3)\n",
    "\n",
    "    # The Decoder is also an LSTM layer with `n_lstm_units` units, but it returns sequences (return_sequences=True)\n",
    "    # instead of the final state: we want to know the output (probabilities) for all the words in the Spanish sentence, not just the last one. \n",
    "    # It cannot be bidirectional, since it needs to generate the words in order (otherwise, it would be cheating).\n",
    "    # Remember that the Decoder is a conditional language model, so it needs to receive the states of the Encoder\n",
    "    # (initial_state parameter)\n",
    "    decoder = tf.keras.layers.LSTM(n_lstm_units_p, return_sequences=True)\n",
    "    decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "    # For each step in the Decoder RNN, we add a Dense layer with a softmax activation function to predict the next word in the Spanish sentence\n",
    "    output_layer = tf.keras.layers.Dense(vocab_size_p, activation=\"softmax\")\n",
    "    Y_probas = output_layer(decoder_outputs)\n",
    "\n",
    "    # Finally, we create the Keras Model, specifying the inputs and outputs\n",
    "    model_loc = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_probas])\n",
    "    \n",
    "    model_loc.summary()\n",
    "    return model_loc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:29.394479Z",
     "start_time": "2024-12-03T16:31:29.383191Z"
    }
   },
   "id": "25dbdaba1d1aa498",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compile and train the model. We use the `sparse_categorical_crossentropy` as the loss function, since the targets are integers (word indices / ids). Otherwise, if we had one-hot vectors, we would use `categorical_crossentropy`. We use the Nadam optimizer and accuracy as a metric. We train the model for a maximum of `n_epochs` epochs, using a batch size of 32. We use early stopping with patience of 2 epochs and restore the best weights.\n",
    "\n",
    "If the model is already saved in the file `model_file_name`, we load it from disk. Otherwise, we compile and train the model and save it to disk.\n",
    "\n",
    "*Notice*: if you run the following cell, you need a GPU (otherwise, it will take more than 3 hours to train the model)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a53d8608fbbd0df"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization (TextVe  (None, 50)                   0         ['input_1[0][0]']             \n",
      " ctorization)                                                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 50, 128)              128000    ['text_vectorization[0][0]']  \n",
      "                                                                                                  \n",
      " text_vectorization_1 (Text  (None, 50)                   0         ['input_2[0][0]']             \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  [(None, 512),                788480    ['embedding[0][0]']           \n",
      " al)                          (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 50, 128)              128000    ['text_vectorization_1[0][0]']\n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)      (None, 512)                  0         ['bidirectional[0][1]',       \n",
      "                                                                     'bidirectional[0][3]']       \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)    (None, 512)                  0         ['bidirectional[0][2]',       \n",
      "                                                                     'bidirectional[0][4]']       \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 50, 512)              1312768   ['embedding_1[0][0]',         \n",
      "                                                                     'tf.concat[0][0]',           \n",
      "                                                                     'tf.concat_1[0][0]']         \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 50, 1000)             513000    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2870248 (10.95 MB)\n",
      "Trainable params: 2870248 (10.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n"
     ]
    }
   ],
   "source": [
    "def compile_and_train_model(model: Model, X_train_encoder_p: np.array, X_train_decoder_p: np.array,\n",
    "                            Y_train_p: np.array, X_valid_encoder_p: np.array, X_valid_decoder_p: np.array,\n",
    "                            Y_valid_p: np.array, n_epochs_p: int, model_file_name: str) -> Model:\n",
    "    if os.path.exists(model_file_name):\n",
    "        return load_model(model_file_name)\n",
    "    # we compile and train the model with sparse_categorical_crossentropy as the loss function, since the targets are integers\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    model.fit((X_train_encoder_p, X_train_decoder_p), Y_train_p,\n",
    "          epochs=n_epochs_p, batch_size=32,\n",
    "          validation_data=((X_valid_encoder_p, X_valid_decoder_p), Y_valid_p),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "    model.save(model_file_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(n_lstm_units, vocab_size)\n",
    "model = compile_and_train_model(model, X_train_encoder, X_train_decoder, Y_train, X_valid_encoder,\n",
    "                                X_valid_decoder, Y_valid, n_epochs, model_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:38.473337Z",
     "start_time": "2024-12-03T16:31:29.396855Z"
    }
   },
   "id": "37c4d582ef4aa4b6",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "We use the model for inference. Now, we use a greedy search strategy to predict the next word in the Spanish sentence. We take the word with the highest probability as the next word. We continue this process until we predict the end of sentence token."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33e3b30400d20f3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone -> [UNK] a todos.\n",
      "how old are you? -> qué edad tienes.\n",
      "what is your name? -> cuál es tu nombre.\n",
      "where are you from? -> de dónde eres.\n",
      "I like soccer -> me gusta el fútbol.\n",
      "This is a too long sentence to be translated correctly -> este es un [UNK] demasiado [UNK] para [UNK] [UNK].\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence_en: str) -> str:\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Spanish, preparing the input for the model and calling the predict method.\n",
    "    :param sentence_en: The English sentence to translate.\n",
    "    :return: The Spanish translation.\n",
    "    \"\"\"\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        # Encoder input: one English sentence (batch size = 1)\n",
    "        X_inf_encoder = np.array([sentence_en])\n",
    "        # Decoder input: SOS + existing translation (empty at the beginning)\n",
    "        X_inf_decoder = np.array([SOS_word + translation])\n",
    "        # We call predict with (Encoder_input, Decoder_input) to get the probabilities of the next word\n",
    "        # we take the first sentence ([0]) and the probabilities idx-th word (returns a list of probabilities for max_length words)\n",
    "        y_probas = model.predict((X_inf_encoder, X_inf_decoder), verbose=0)[0, word_idx]  # probas of the last predicted word\n",
    "        # we take the word id with the highest probability\n",
    "        predicted_word_id = np.argmax(y_probas)\n",
    "        # we get the word from the vocabulary\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == EOS_word:\n",
    "            # we are done when we predict the end of sentence token\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()\n",
    "\n",
    "\n",
    "# we test the translation with some sentences. Feel free to add more sentences to test the model\n",
    "english_sentences = [\"hello everyone\",\n",
    "                     \"how old are you?\",\n",
    "                     \"what is your name?\",\n",
    "                     \"where are you from?\",\n",
    "                     \"I like soccer\",\n",
    "                     \"This is a too long sentence to be translated correctly\"]\n",
    "for sentence in english_sentences:\n",
    "    print(f\"{sentence} -> {translate(sentence)}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T16:31:46.494909Z",
     "start_time": "2024-12-03T16:31:38.477229Z"
    }
   },
   "id": "e211e62fe67248ac",
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
