{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/encoder_decoder.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "87540b40e3958a06"
   },
   "id": "87540b40e3958a06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder-Decoder architecture\n",
    "\n",
    "The [Encoder-Decoder architecture](https://medium.com/analytics-vidhya/encoders-decoders-sequence-to-sequence-architecture-5644efbb3392) is a neural network architecture used in sequence-to-sequence (Seq2Seq) tasks. It is composed of two main parts: the encoder and the decoder. The encoder processes the input sequence and compresses it into a fixed-size internal representation (hidden state of context vector). The decoder is a conditional language model that generates the output sequence.\n",
    "\n",
    "On of the common usages of the Encoder-Decoder architecture is in [neural machine translation](https://en.wikipedia.org/wiki/Neural_machine_translation), where the input sequence is a sentence in one language and the output sequence is the translation of the sentence in another language. The encoder processes the input sentence and compresses it into a fixed-size internal representation. The decoder generates the translation of the sentence in the target language.\n",
    "\n",
    "In this notebook, we will implement a simple Encoder-Decoder architecture using Recurrent Neural Networks (RNN) to translate English into Spanish. The Enoder is a simple bidirectional LSTM network, and the decoder is a simple LSTM network. \n",
    "\n",
    "<img src=\"img/encoder-decoder.jpg\" width=\"1200\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af819f8c13c3ccfa"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f099c66705669f8",
   "metadata": {
    "id": "5f099c66705669f8",
    "ExecuteTime": {
     "end_time": "2024-12-04T08:41:32.320713Z",
     "start_time": "2024-12-04T08:41:30.663723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/rnn'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !pip uninstall -y keras --quiet\n",
    "    !pip install keras==2.15.0 --quiet\n",
    "    !pip install tensorflow==2.15.1 --quiet\n",
    "    !git clone --depth 1 https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "    !cp {directory}/img/* img/.\n",
    "\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important variables\n",
    "\n",
    "We define the following variables:\n",
    "- `vocab_size`: the size of the vocabulary (number of unique words in both English and Spanish languages).\n",
    "- `max_length`: the maximum length of the input and output sequences (in words). If a sequence is longer than this, it will be truncated. If it is shorter, it will be padded.\n",
    "- `chars_to_remove`: a list of characters to remove from the text.\n",
    "- `train_size_percentage`: the percentage of the data to use for training ([0-100]).\n",
    "- `embedding_size`: the size of the embedding layer (hyperparameter).\n",
    "- `n_epochs`: the maximum number of epochs to train the model (early stopping is used).\n",
    "- `SOS_word`, `EOS_word`: the start and end of sentence special words.\n",
    "- `n_lstm_units`: the number of LSTM units in the Encoder and Decoder RNNs.\n",
    "- `model_file_name`: the file name to save or load the trained model. If the file exists, the model is loaded from disk, otherwise, the model is trained and saved."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65cd51ca9dd147d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab_size = 5_000\n",
    "max_length = 50\n",
    "chars_to_remove = [\"¡\", \"¿\"]\n",
    "train_size_percentage = 85\n",
    "embedding_size = 128\n",
    "n_epochs = 10\n",
    "n_lstm_units = 512\n",
    "SOS_word, EOS_word = \"startofsentence\", \"endofsentence\"\n",
    "model_file_name = 'data/english_spanish_encoder_decoder.keras'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:41:32.330761Z",
     "start_time": "2024-12-04T08:41:32.324225Z"
    }
   },
   "id": "958e5ce1e4e8001c",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data\n",
    "\n",
    "Our dataset is a collection of 118,964 English-Spanish sentence pairs taken from [here](https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip). We load the file and remove special characters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e6ac944f5fef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read the contents of the data/english-spanish.txt file\n",
    "with open(\"data/english-spanish.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "# remove the special characters\n",
    "for special_char in chars_to_remove:\n",
    "    text = text.replace(special_char, \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:41:32.401206Z",
     "start_time": "2024-12-04T08:41:32.332919Z"
    }
   },
   "id": "e14a73f497875d41",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get a list of the English and Spanish sentences by splitting each line by the tab character. We shuffle the list of pairs, convert it to a pair of lists and show some examples in them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b5e3f3027b4b20"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1,000.\n",
      "Some example translations:\n",
      "\t1: He doesn't have as many books as she does. -> No tiene tantos libros como ella.\n",
      "\t2: I asked Tom a question. -> Le hice una pregunta a Tom.\n",
      "\t3: Young tigers resemble cats. -> Las crías de tigre parecen gatos.\n",
      "\t4: I first met him three years ago. -> Lo conocí hace tres años.\n",
      "\t5: Apparently, the bus is late. -> Aparentemente, el autobús está retrasado.\n"
     ]
    }
   ],
   "source": [
    "# take the English and Spanish sentences, by splitting each line by the tab character\n",
    "pairs: list[(str, str)] = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "\n",
    "# take a list of pairs and returns a pair of lists: one with the English sentences and one with the Spanish sentences\n",
    "sentences_en, sentences_es = zip(*pairs)\n",
    "\n",
    "sentences_en = sentences_en[:1000]\n",
    "sentences_es = sentences_es[:1000]\n",
    "\n",
    "assert (n_sentences := len(sentences_en)) == len(sentences_es)\n",
    "print(f\"Number of sentences: {n_sentences:,}.\")\n",
    "\n",
    "print(\"Some example translations:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\t{i+1}: {sentences_en[i]} -> {sentences_es[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:41:33.788420Z",
     "start_time": "2024-12-04T08:41:32.403453Z"
    }
   },
   "id": "6ed0b0a854205a7d",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Encoder-Decoder ANN has two inputs: one for the Encoder (English) and another one for the Decoder (Spanish). Both are strings. However, we create a `TextVectorization` layer for each input, which transforms a batch of strings into a list of token indices or ids (ints). Upon creation, we pass the vocabulary size and the maximum length of the sequences.  \n",
    "\n",
    "Word index/id definition is performed with the `adapt` method, which transforms each input sentence into a list of word indices, considering the vocabulary size. The most frequent words will be mapped to the first token indices, and the least frequent words to the last token indices. Those with a frequency below the vocabulary size will be mapped to the same token index [UNK].\n",
    "\n",
    "For the Decoder input (Spanish sentences), we include the start and end of sentence tokens (SOS and EOS). SOS will indicate the Decoder to start generating the first Spanish word, and EOS will indicate the end of the sentence (termination of generation)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0c8581c7e4dd658"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some example English words: ['', '[UNK]', 'i', 'to', 'the', 'tom', 'you', 'a', 'is', 'he']\n",
      "Some example Spanish words: ['', '[UNK]', 'startofsentence', 'endofsentence', 'tom', 'no', 'de', 'a', 'que', 'el']\n"
     ]
    }
   ],
   "source": [
    "# TextVectorization is as keras layer that converts a batch of strings into either a list of token indices / ids (ints)\n",
    "# It could also output a dense representation of the strings, where each token is represented by a dense vector (not used here)\n",
    "# sentences longer than `max_length` are truncated, and shorter sentences are padded with zeros\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "\n",
    "# adapt makes the layer to transform each input sentence into a list of word indices, considering the vocabulary size\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "# we adapt the Spanish  layer to the Spanish sentences, including the start and end of sentence tokens\n",
    "text_vec_layer_es.adapt([f\"{SOS_word} {sentence} {EOS_word}\" for sentence in sentences_es])\n",
    "\n",
    "print(f\"Some example English words: {text_vec_layer_en.get_vocabulary()[:10]}\")  # 0 is padding, visualized as ''\n",
    "print(f\"Some example Spanish words: {text_vec_layer_es.get_vocabulary()[:10]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:41:34.246619Z",
     "start_time": "2024-12-04T08:41:33.792625Z"
    }
   },
   "id": "da62a9a2f125d58",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the data into training and validation sets (both for Encoder and Decoder inputs). We include the start of sentence token at the beginning of each sentence. The output of the Decoder is the same as the input, but shifted one position to the right, and with the end of sentence token at the end of each sentence.\n",
    "\n",
    "The input is one full English sentence (e.g., \"I like soccer\") for the Encoder and the corresponding Spanish sentence prefixed with SOS (e.g., \"SOS Me gusta el fútbol\") for the Decoder. The output is the Spanish sentence without SOS and with EOS at the end (e.g., \"Me gusta el fútbol EOS\"). In this way, give \"I like soccer\" to the Encoder, and SOS to the Decoder, the latter will generate \"Me\". This output will be passed again to the Decoder concatenated to the previous input (i.e., \"SOS Me\"), which will generate \"gusta\", and so on, until it generates the EOS."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9eda0d14cb02e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# we split the data into training and validation sets\n",
    "# we first take the input for the Encoder (English sentences)\n",
    "X_train_encoder = tf.constant(sentences_en[:n_sentences * train_size_percentage // 100])\n",
    "X_valid_encoder = tf.constant(sentences_en[n_sentences*train_size_percentage//100:])\n",
    "\n",
    "# then, we take the input for the Decoder (Spanish sentences)\n",
    "# We include the SOS at the beginning of each sentence. This is because we want the Decoder to start generating\n",
    "# the first Spanish word, by passing SOS as the first input. Then, the Decoder will generate the first word and\n",
    "# we will pass it to the Decoder again, so it can generate the second word, and so on, until it generates the EOS.\n",
    "# EOS does not need to be added to the input, since we want the Decoder to generate it (it will be added to\n",
    "# Y training dataset).\n",
    "X_train_decoder = tf.constant([f\"{SOS_word} {sentence}\" for sentence in\n",
    "                               sentences_es[:n_sentences * train_size_percentage // 100]])\n",
    "X_valid_decoder = tf.constant([f\"{SOS_word} {sentence}\" for sentence in\n",
    "                               sentences_es[n_sentences * train_size_percentage // 100:]])\n",
    "\n",
    "# The output of the Decoder is the same as the input, but shifted one position to the right, and with EOS at the end\n",
    "# of each sentence. This is because we want the Decoder to generate the first word of the Spanish sentence, then\n",
    "# the second word, and so on, until it generates the EOS.\n",
    "Y_train = text_vec_layer_es([f\"{sentence} {EOS_word}\" for sentence in sentences_es[:n_sentences * train_size_percentage // 100]])\n",
    "Y_valid = text_vec_layer_es([f\"{sentence} {EOS_word}\" for sentence in sentences_es[n_sentences * train_size_percentage // 100:]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:41:34.298261Z",
     "start_time": "2024-12-04T08:41:34.249926Z"
    }
   },
   "id": "fe9335f6522bdf48",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the model\n",
    "\n",
    "Let's create the Encoder-Decoder ANN with two recurrent neural networks (RNNs). The Encoder is a bidirectional LSTM network, and the Decoder is a simple LSTM network. The Encoder processes the input sequence and compresses it into a fixed-size internal representation. The Decoder generates the output sequence. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25d3b8c50f945417"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_model(n_lstm_units_p: int, vocab_size_p: int) -> Model:\n",
    "    \"\"\"\n",
    "    Creates a Keras model for the Encoder-Decoder architecture.\n",
    "    :param n_lstm_units_p: Number of LSTM units in the Encoder and Decoder.\n",
    "    :param vocab_size_p: Vocabulary size.\n",
    "    :return: The model\n",
    "    \"\"\"\n",
    "    # Both the Encoder and the Decoder will receive a batch of sentences (strings) as input \n",
    "    # (English sentences for the Encoder, and Spanish sentences for the Decoder).\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "    # We connect the inputs t the text vectorization layers using the Keras functional API\n",
    "    # In this way, the input sequences are converted into lists of word indices / ids using the TextVectorization layers\n",
    "    encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "    decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "    # The word indices are then converted into dense vectors using an Embedding layer of `embedding_size` dimensions\n",
    "    # The padding character zero is masked out, so it is ignored by the model (its weight is not updated/learned). This speeds up training.\n",
    "    encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size_p, embedding_size, mask_zero=True)\n",
    "    decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size_p, embedding_size, mask_zero=True)\n",
    "    # we connect the embedding layers to the input indices (functional API)\n",
    "    encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "    decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n",
    "\n",
    "    # We create the Encoder as a single bidirectional LSTM layer with half of the units (two LSTMs, one for each direction)\n",
    "    # Return_state=True => gets a reference to the layer’s final state (not the output for all the RNN steps)\n",
    "    # Since we are using a bi-LSTM layer, the final state is a tuple containing 2 short- and 2 long-term states,\n",
    "    # one pair for each direction (that is why we use *encoder_states, to store the four states in one single tuple variable)\n",
    "    encoder = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(n_lstm_units_p // 2, return_state=True))\n",
    "    encoder_outputs, *encoder_states = encoder(encoder_embeddings)\n",
    "\n",
    "    # we concatenate the states of the left and right LSTMs (first, the 2 short-term states and then the 2 long-term states)\n",
    "    # this way, we get a single state for each type (short and long-term) to be passed \n",
    "    # to the one-directional Decoder RNN as its initial state (conditional language model)\n",
    "    encoder_state = (keras.layers.Concatenate(axis=1)([encoder_states[0], encoder_states[2]]), # short-term (0 & 2)\n",
    "                     keras.layers.Concatenate(axis=1)([encoder_states[1], encoder_states[3]])) # long-term (1 & 3)\n",
    "\n",
    "    # The Decoder is also an LSTM layer with `n_lstm_units` units, but it returns sequences (return_sequences=True)\n",
    "    # instead of the final state: we want to know the output (probabilities) for all the words in the Spanish sentence, not just the last one. \n",
    "    # It cannot be bidirectional, since it needs to generate the words in order (otherwise, it would be cheating).\n",
    "    # Remember that the Decoder is a conditional language model, so it needs to receive the states of the Encoder\n",
    "    # (initial_state parameter)\n",
    "    decoder = tf.keras.layers.LSTM(n_lstm_units_p, return_sequences=True)\n",
    "    decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "    # For each step in the Decoder RNN, we add a Dense layer with a softmax activation function to predict the next word in the Spanish sentence\n",
    "    output_layer = tf.keras.layers.Dense(vocab_size_p, activation=\"softmax\")\n",
    "    Y_probas = output_layer(decoder_outputs)\n",
    "\n",
    "    # Finally, we create the Keras Model, specifying the inputs and outputs\n",
    "    model_loc = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_probas])\n",
    "    \n",
    "    model_loc.summary()\n",
    "    return model_loc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:41:34.309217Z",
     "start_time": "2024-12-04T08:41:34.300688Z"
    }
   },
   "id": "25dbdaba1d1aa498",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compile and train the model. We use the `sparse_categorical_crossentropy` as the loss function, since the targets are integers (word indices / ids). Otherwise, if we had one-hot vectors, we would use `categorical_crossentropy`. We use the Nadam optimizer and accuracy as a metric. We train the model for a maximum of `n_epochs` epochs, using a batch size of 32. We use early stopping with patience of 2 epochs and restore the best weights.\n",
    "\n",
    "If the model is already saved in the file `model_file_name`, we load it from disk. Otherwise, we compile and train the model and save it to disk.\n",
    "\n",
    "*Notice*: if you run the following cell, you need a GPU (otherwise, it will take more than 3 hours to train the model)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a53d8608fbbd0df"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)       [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization_6 (Text  (None, 50)                   0         ['input_11[0][0]']            \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)       [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)    (None, 50, 128)              640000    ['text_vectorization_6[0][0]']\n",
      "                                                                                                  \n",
      " text_vectorization_7 (Text  (None, 50)                   0         ['input_12[0][0]']            \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirecti  [(None, 512),                788480    ['embedding_10[0][0]']        \n",
      " onal)                        (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)    (None, 50, 128)              640000    ['text_vectorization_7[0][0]']\n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 512)                  0         ['bidirectional_5[0][1]',     \n",
      " )                                                                   'bidirectional_5[0][3]']     \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate  (None, 512)                  0         ['bidirectional_5[0][2]',     \n",
      " )                                                                   'bidirectional_5[0][4]']     \n",
      "                                                                                                  \n",
      " lstm_11 (LSTM)              (None, 50, 512)              1312768   ['embedding_11[0][0]',        \n",
      "                                                                     'concatenate_6[0][0]',       \n",
      "                                                                     'concatenate_7[0][0]']       \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 50, 5000)             2565000   ['lstm_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5946248 (22.68 MB)\n",
      "Trainable params: 5946248 (22.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "27/27 [==============================] - 38s 835ms/step - loss: 7.0202 - accuracy: 0.1346 - val_loss: 6.2089 - val_accuracy: 0.1483\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 17s 613ms/step - loss: 5.6960 - accuracy: 0.1686 - val_loss: 6.1440 - val_accuracy: 0.1721\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 18s 669ms/step - loss: 5.4011 - accuracy: 0.1774 - val_loss: 6.0861 - val_accuracy: 0.1787\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 17s 648ms/step - loss: 5.2009 - accuracy: 0.1799 - val_loss: 6.0917 - val_accuracy: 0.1816\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 19s 703ms/step - loss: 5.0495 - accuracy: 0.1824 - val_loss: 6.1386 - val_accuracy: 0.1749\n"
     ]
    }
   ],
   "source": [
    "def compile_and_train_model(model: Model, X_train_encoder_p: np.array, X_train_decoder_p: np.array,\n",
    "                            Y_train_p: np.array, X_valid_encoder_p: np.array, X_valid_decoder_p: np.array,\n",
    "                            Y_valid_p: np.array, n_epochs_p: int, model_file_name: str) -> Model:\n",
    "    if os.path.exists(model_file_name):\n",
    "        return load_model(model_file_name)\n",
    "    # we compile and train the model with sparse_categorical_crossentropy as the loss function, since the targets are integers\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    model.fit((X_train_encoder_p, X_train_decoder_p), Y_train_p,\n",
    "          epochs=n_epochs_p, batch_size=32,\n",
    "          validation_data=((X_valid_encoder_p, X_valid_decoder_p), Y_valid_p),\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)])\n",
    "    model.save(model_file_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(n_lstm_units, vocab_size)\n",
    "model = compile_and_train_model(model, X_train_encoder, X_train_decoder, Y_train, X_valid_encoder,\n",
    "                                X_valid_decoder, Y_valid, n_epochs, model_file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:43:27.308745Z",
     "start_time": "2024-12-04T08:41:34.311417Z"
    }
   },
   "id": "37c4d582ef4aa4b6",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "We use the model for inference. Now, we use a greedy search strategy to predict the next word in the Spanish sentence. We take the word with the highest probability as the next word. We continue this process until we predict the end of sentence token."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33e3b30400d20f3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone -> .\n",
      "how old are you? -> tom es.\n",
      "what is your name? -> tom es.\n",
      "where are you from? -> tom de.\n",
      "I like soccer -> tom.\n",
      "This is a too long sentence to be translated correctly -> tom no no que que que de de.\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence_en: str) -> str:\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Spanish, preparing the input for the model and calling the predict method.\n",
    "    :param sentence_en: The English sentence to translate.\n",
    "    :return: The Spanish translation.\n",
    "    \"\"\"\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        # Encoder input: one English sentence (batch size = 1)\n",
    "        X_inf_encoder = np.array([sentence_en])\n",
    "        # Decoder input: SOS + existing translation (empty at the beginning)\n",
    "        X_inf_decoder = np.array([SOS_word + translation])\n",
    "        # We call predict with (Encoder_input, Decoder_input) to get the probabilities of the next word\n",
    "        # we take the first sentence ([0]) and the probabilities idx-th word (returns a list of probabilities for max_length words)\n",
    "        y_probas = model.predict((X_inf_encoder, X_inf_decoder), verbose=0)[0, word_idx]  # probas of the last predicted word\n",
    "        # we take the word id with the highest probability\n",
    "        predicted_word_id = np.argmax(y_probas)\n",
    "        # we get the word from the vocabulary\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == EOS_word:\n",
    "            # we are done when we predict the end of sentence token\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "    return translation.strip()\n",
    "\n",
    "\n",
    "# we test the translation with some sentences. Feel free to add more sentences to test the model\n",
    "english_sentences = [\"hello everyone\",\n",
    "                     \"how old are you?\",\n",
    "                     \"what is your name?\",\n",
    "                     \"where are you from?\",\n",
    "                     \"I like soccer\",\n",
    "                     \"I want you to try to correctly translate this long sentence from English to Spanish\"]\n",
    "for sentence in english_sentences:\n",
    "    print(f\"{sentence} -> {translate(sentence)}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:43:35.093447Z",
     "start_time": "2024-12-04T08:43:27.312687Z"
    }
   },
   "id": "e211e62fe67248ac",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨\n",
    "\n",
    "1. In the training data \"I like\" is mostly translated as \"Me gustan\". However, the model translates \"I like\" as \"Me gusta\". Why is that?\n",
    "2. Is there any sentence that is not translated correctly? \n",
    "3. If so, why do you think that happens?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98289b5ebf0d9e9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n",
    "\n",
    "1. It is because the Encoder RNN is biderectional and detects that \"soccer\", which the next word, is singular. For a one-directional RNN, it would be translated as \"Me gustan\".\n",
    "2. Yes, the last sentence is not translated correctly.\n",
    "3. RNN-based Encoder-Decoder models have a limited memory, so they may not be able to remember the first and last words of a long sentence (the ones in the middle in a bidirectional RNN) to translate it correctly. Remember that the Decoder is a conditional language model, so the hidden state it receives must store the semantics of the whole input sentence in the Encoder. This is a limitation of RNNs that is improved with the use of attention.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52ae6a1632861faa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Beam search\n",
    "\n",
    "The previous greedy search strategy is not the best one. I may take a word with the highest probability, but it may not be the best probability for any second word. That is, $P(w_1, w_2)$ may not be as high as $P(w_1', w_2')$. This is the problem of finding a local maximum instead of a global maximum. Performing an exact search is not tractable, since the number of possible sentences grows exponentially with the length of the sentence.\n",
    "\n",
    "We can use the Beam search strategy, which considers the $k$ most probable words at each step. We keep track of the $k$ most probable sequences and continue the process until we predict the end of sentence token. We take the sequence with the highest probability at the end."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "534fd211a92825c0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Translation with beam search for: \n",
      "\t 'hello everyone':\n",
      "Top first words: [(-5.7721334, 'endofsentence'), (-6.027619, 'a'), (-6.042031, 'de')]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 65\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m50\u001B[39m)\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTranslation with beam search for: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msentence\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 65\u001B[0m translation \u001B[38;5;241m=\u001B[39m \u001B[43mbeam_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpanish translation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtranslation\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[39], line 47\u001B[0m, in \u001B[0;36mbeam_search\u001B[1;34m(sentence_en, beam_width, verbose)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;66;03m# we include in candidates the top k existing translations with all the possible next words and their probabilities\u001B[39;00m\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word_id, word_proba \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(y_proba):\n\u001B[1;32m---> 47\u001B[0m         word \u001B[38;5;241m=\u001B[39m \u001B[43mtext_vec_layer_es\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_vocabulary\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mword_id\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     48\u001B[0m         candidates\u001B[38;5;241m.\u001B[39mappend((log_proba \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(word_proba), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtranslation\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mword\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# we sort the candidates by the log of the probabilities and take the top k\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def beam_search(sentence_en: str, beam_width: int, verbose=False) -> str:\n",
    "    \"\"\"\n",
    "    Translates an English sentence into Spanish using beam search wit k=beam_width.\n",
    "    :param sentence_en: The Encoder input (English sentence).\n",
    "    :param beam_width: The k parameter of the beam search algorithm.\n",
    "    :param verbose: Whether to display the top words and translations at each step.\n",
    "    :return: The Spanish translation.\n",
    "    \"\"\"\n",
    "    # Translation of the first word\n",
    "    # Encoder input: one English sentence (batch size = 1)\n",
    "    X_inf_encoder = np.array([sentence_en])\n",
    "    # Decoder input: SOS\n",
    "    X_inf_decoder = np.array([SOS_word])\n",
    "    # Predict the probabilities of the first word\n",
    "    y_proba = model.predict((X_inf_encoder, X_inf_decoder), verbose=0)[0, 0]  # first token's probas\n",
    "    # we take the top k words with the highest probabilities Dict{word_id: proba}\n",
    "    top_k_words = tf.math.top_k(y_proba, k=beam_width)\n",
    "    # list of best (log_proba, translation) pairs\n",
    "    # Important: instead of taking Prob(w1) * Prob(w2) * ... * Prob(wn), we take the log of the product:\n",
    "    # log(Prob(w1)) + log(Prob(w2)) + ... + log(Prob(wn))\n",
    "    # this is because the product of many probabilities between 0 and 1 can be very small and lead to 0.0 after some iterations\n",
    "    top_translations = [\n",
    "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
    "        for word_proba, word_id in zip(top_k_words.values, top_k_words.indices)\n",
    "    ]\n",
    "\n",
    "    # displays the top first words if verbose mode\n",
    "    print(\"Top first words:\", top_translations) if verbose else None\n",
    "\n",
    "    # Translation of the next words (from 1 on)\n",
    "    for idx in range(1, max_length):\n",
    "        # list of best (log_proba, translation) pairs\n",
    "        candidates: list[(float, str)] = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith(EOS_word):\n",
    "                candidates.append((log_proba, translation))\n",
    "                # translation is finished, so don't try to extend it\n",
    "                continue\n",
    "            # Encoder input: one English sentence (batch size = 1)\n",
    "            X_inf_encoder = np.array([sentence_en])  # encoder input\n",
    "            # Decoder input: SOS + existing translation\n",
    "            X_inf_decoder = np.array([SOS_word + \" \" + translation])  # decoder input\n",
    "            # probabilites of the new word\n",
    "            y_proba = model.predict((X_inf_encoder, X_inf_decoder), verbose=0)[0, idx]  # last token's proba\n",
    "            # we include in candidates the top k existing translations with all the possible next words and their probabilities\n",
    "            for word_id, word_proba in enumerate(y_proba):\n",
    "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
    "                candidates.append((log_proba + np.log(word_proba), f\"{translation} {word}\"))\n",
    "        # we sort the candidates by the log of the probabilities and take the top k\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "\n",
    "        # displays the top translation so far, if verbose mode\n",
    "        print(\"Top translations so far:\", top_translations) if verbose else None\n",
    "\n",
    "        # the process terminates when all the K top translations end with the EOS token\n",
    "        if all([top_translation.endswith(EOS_word) for _, top_translation in top_translations]):\n",
    "            # returns the best translation pair ([0] because it is sorted by log probabilities),\n",
    "            # take the translation text ([1]) and remove the EOS token\n",
    "            return top_translations[0][1].replace(EOS_word, \"\").strip()\n",
    "\n",
    "\n",
    "for sentence in english_sentences:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Translation with beam search for: \\n\\t '{sentence}':\")\n",
    "    translation = beam_search(sentence, 3, verbose=True)\n",
    "    print(f\"Spanish translation: {translation}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T08:43:40.125876Z",
     "start_time": "2024-12-04T08:43:35.095705Z"
    }
   },
   "id": "18013c05d8de4904",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "This simple model performs decently on short sentences, but it struggles with longer sentences. It is possible to significantly improve the translation quality by using [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)). A more sophisticated implementation of the Encoder-Decoder architecture with attention called [Transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) is the state-of-the-art model for machine translation, currently used by GPT, BERT, and many other models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "969d8f28560f892a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨\n",
    "\n",
    "3. What would happen in beam search if we used probability product instead of the sum of log probabilities?\n",
    "4. Do you think the last sentence will be translated better with $k$=10? Try it out."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21d3d1b2ff382850"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "\n",
    "*Write your answers here.*\n",
    "\n",
    "3. The product of many probabilities between 0 and 1 can be very small and lead to 0.0 after some iterations.\n",
    "4. It might be because it is long and there might be a better global solution. However, we tried it out and the translation is not better with $k$=10."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73be660cdce9ff09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
