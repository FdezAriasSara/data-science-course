{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/sentiment.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbf6d0671dafefcc"
  },
  {
   "cell_type": "markdown",
   "id": "72ea15f3d70547f7",
   "metadata": {
    "collapsed": false,
    "id": "72ea15f3d70547f7"
   },
   "source": [
    "# Sentiment classification with RNNs\n",
    "\n",
    "In this notebook, we implement a sentiment analysis task to classify movie reviews as positive or negative. By analyzing the text of written by the users, we will predict the sentiment of each review. We use the the [IMDb dataset](https://keras.io/api/datasets/imdb/), a set of 50,000 movie reviews from the [Internet Movie Database](https://en.wikipedia.org/wiki/IMDb) (IMDb).\n",
    "\n",
    "<img src=\"img/imdb.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f099c66705669f8",
   "metadata": {
    "id": "5f099c66705669f8",
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:10.494374Z",
     "start_time": "2024-11-20T16:05:07.512196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow tensorflow-hub --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/rnn'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone --depth 1 https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "    !cp {directory}/img/* img/.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3e8a3ff34b7dc",
   "metadata": {
    "collapsed": false,
    "id": "e7c3e8a3ff34b7dc"
   },
   "source": [
    "## Load the IMDb dataset\n",
    "\n",
    " We use the 5,000 most frequent words in the dataset. We cut the reviews to a maximum length of 80 words to speed up training. We use embeddings of 1024 dimensions and a large number of epochs (50) to train the models because we use early stopping. We only consider 1,000 sentences of the IMDb dataset because of memory restrictions (lower this number if you have memory issues)."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Consider all the words with a frequency higher than this value. The higher, the more memory is needed.\n",
    "vocabulary_size = 5_000\n",
    "# Compute the maximum length of the reviews (for speeding up the training it is better to cut the reviews)\n",
    "max_review_length = 80\n",
    "# Max number of epochs to train the models (we use early stopping)\n",
    "n_epochs = 50\n",
    "# Embedding dimensions (ELMo embeddings have 1024 dimensions)\n",
    "embedding_dim = 1024\n",
    "# Max number of sentences for training. Reduce this number if you do not have enough memory.\n",
    "max_sentences_train = 1_000"
   ],
   "metadata": {
    "id": "f5f7eb0c67d58ee5",
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:10.509132Z",
     "start_time": "2024-11-20T16:05:10.499293Z"
    }
   },
   "id": "f5f7eb0c67d58ee5",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the dataset from the Keras API. Half of the reviews are used for training, and the other half for validation and testing."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c50a52ab52105e74"
   },
   "id": "c50a52ab52105e74"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: 1,000.\n",
      "Validation sequences: 500.\n",
      "Testing sequences: 500.\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_half, y_half) = keras.datasets.imdb.load_data(num_words=vocabulary_size)\n",
    "X_train, y_train = X_train[:max_sentences_train], y_train[:max_sentences_train]\n",
    "X_half, y_half = X_half[:max_sentences_train], y_half[:max_sentences_train]\n",
    "# get validation set as half of the test set\n",
    "X_test, X_val = X_half[:len(X_half) // 2], X_half[len(X_half) // 2:]\n",
    "y_test, y_val = y_half[:len(y_half) // 2], y_half[len(y_half) // 2:]\n",
    "X_half, y_half = None, None  # free memory\n",
    "\n",
    "print(f\"Training sequences: {len(X_train):,}.\\nValidation sequences: {len(X_val):,}.\\nTesting sequences: {len(X_test):,}.\")"
   ],
   "metadata": {
    "id": "73a4ffd175d95c8b",
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:13.850192Z",
     "start_time": "2024-11-20T16:05:10.513130Z"
    }
   },
   "id": "73a4ffd175d95c8b",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process the dataset\n",
    "\n",
    "We store all the word indexes returned by the IMDb dataset in a dictionary. An `index_to_word` dictionary is created to convert the token IDs back to words. We reserve the first four indices for special tokens `<PAD>`, `<START>`, `<OOV>`, and `<END>`."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5f82fcb2a4d25d87"
   },
   "id": "5f82fcb2a4d25d87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's print some reviews. We need to convert the integers (token ids) back to words.\n",
    "word_to_index = {word: index+3 for word, index in keras.datasets.imdb.get_word_index().items()}  # word -> integer dictionary\n",
    "# The IMDB dataset reserves the 4 first indices for special tokens <PAD>, <START>, <OOV>, <END>\n",
    "index_to_word = {value: key for key, value in word_to_index.items()}  # integer -> word dictionary\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<START>\"\n",
    "index_to_word[2] = \"<OOV>\"\n",
    "index_to_word[3] = \"<END>\""
   ],
   "metadata": {
    "id": "c98ebd5c105cfde3",
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:13.971766Z",
     "start_time": "2024-11-20T16:05:13.855721Z"
    }
   },
   "id": "c98ebd5c105cfde3",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "We show the first reviews and their corresponding sentiment. "
   ],
   "metadata": {
    "collapsed": false,
    "id": "d564187c851e6ad8"
   },
   "id": "d564187c851e6ad8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First reviews in training set, with the corresponding labels:\n",
      "Review 1: <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <OOV> is an amazing actor and now the same being director <OOV> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <OOV> and would recommend it to everyone to watch and the fly <OOV> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <OOV> to the two little <OOV> that played the <OOV> of norman and paul they were just brilliant children are often left out of the <OOV> list i think because the stars that play them all grown up are such a big <OOV> for the whole film but these children are amazing and should be <OOV> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <OOV> with us all.\n",
      "Label: 1.\n",
      "Review 2: <START> big hair big <OOV> bad music and a giant safety <OOV> these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an <OOV> the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are <OOV> and funny in equal <OOV> the hair is big lots of <OOV> <OOV> men wear those cut <OOV> <OOV> that show off their <OOV> <OOV> that men actually wore them and the music is just <OOV> trash that plays over and over again in almost every scene there is trashy music <OOV> and <OOV> taking away bodies and the <OOV> still doesn't close for <OOV> all <OOV> aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then.\n",
      "Label: 0.\n",
      "Review 3: <START> this has to be one of the worst films of the <OOV> when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <OOV> working to watch this <OOV> excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <OOV> this is to watch save yourself an hour a bit of your life.\n",
      "Label: 0.\n",
      "Review 4: <START> the <OOV> <OOV> at storytelling the traditional sort many years after the event i can still see in my <OOV> eye an elderly lady my <OOV> mother <OOV> the battle of <OOV> she makes the characters come alive her passion is that of an eye witness one to the events on the <OOV> <OOV> a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in <OOV> the length and <OOV> of <OOV> as i <OOV> it with a friend one night in <OOV> a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional <OOV> fact and fiction blend with <OOV> role models warning stories <OOV> magic and mystery br br my name is <OOV> like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that <OOV> back through <OOV> it <OOV> stories within stories stories that <OOV> the <OOV> wonder of <OOV> its <OOV> mountains <OOV> in <OOV> the stuff of legend yet <OOV> is <OOV> in reality this is what gives it its special charm it has a rough beauty and <OOV> <OOV> with some of the finest <OOV> singing you will ever hear br br <OOV> <OOV> visits his grandfather in hospital shortly before his death he burns with frustration part of him <OOV> to be in the twenty first century to hang out in <OOV> but he is raised on the western <OOV> among a <OOV> speaking community br br yet there is a deeper conflict within him he <OOV> to know the truth the truth behind his <OOV> ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last <OOV> journey to the <OOV> of one of <OOV> most <OOV> mountains can the truth be told or is it all in stories br br in this story about stories we <OOV> bloody battles <OOV> lovers the <OOV> of old and the sometimes more <OOV> <OOV> of accepted truth in doing so we each connect with <OOV> as he lives the story of his own life br br <OOV> the <OOV> <OOV> is probably the most honest <OOV> and genuinely beautiful film of <OOV> ever made like <OOV> i got slightly annoyed with the <OOV> of hanging stories on more stories but also like <OOV> i <OOV> this once i saw the <OOV> picture ' forget the box office <OOV> of <OOV> and its like you might even <OOV> the <OOV> famous <OOV> of the <OOV> man to see a film that is true to <OOV> this one is probably unique if you maybe <OOV> on it deeply enough you might even re <OOV> the power of storytelling and the age old question of whether there are some <OOV> that cannot be told but only experienced.\n",
      "Label: 1.\n",
      "Review 5: <START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon <OOV> my head in i stuck that damn movie in the <OOV> and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and <OOV> a whole day of my life.\n",
      "Label: 0.\n"
     ]
    }
   ],
   "source": [
    "def decode_review(encoded_review: list[int]) -> str:\n",
    "    \"\"\"Decode a review from a list of integers to a string.\"\"\"\n",
    "    return ' '.join(index_to_word.get(word_index, \"<OOV>\") for word_index in encoded_review)\n",
    "\n",
    "print(\"First reviews in training set, with the corresponding labels:\")\n",
    "for (i, (review, label)) in enumerate(zip(X_train[:5], y_train[:5])):\n",
    "    print(f\"Review {i + 1}: {decode_review(review)}.\\nLabel: {label}.\")"
   ],
   "metadata": {
    "id": "43ef1964ad0dad2",
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:13.980141Z",
     "start_time": "2024-11-20T16:05:13.972821Z"
    }
   },
   "id": "43ef1964ad0dad2",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add padding to the reviews to have the same length. We use the `post` mode to pad and truncate at the end of the reviews."
   ],
   "metadata": {
    "collapsed": false,
    "id": "be41a096e9dbefa9"
   },
   "id": "be41a096e9dbefa9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pad the reviews to have the same length (padding and truncating at the end with \"post\")\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length, padding=\"post\", truncating=\"post\")"
   ],
   "metadata": {
    "id": "419ca03571ca5b6d",
    "outputId": "36540554-d259-4ae4-d30c-e76419c7708a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:14.130627Z",
     "start_time": "2024-11-20T16:05:13.981716Z"
    }
   },
   "id": "419ca03571ca5b6d",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-directional LSTM model\n",
    "\n",
    "We create a one-direction LSTM RNN where embeddings are computed as the first layer using the Keras `Embedding` layer. We have to choose the size of the embedding vectors (hyperparameter)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2c21cb3cd6d19e51"
   },
   "id": "2c21cb3cd6d19e51"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 1024)        5120000   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 64)                278784    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5398849 (20.59 MB)\n",
      "Trainable params: 5398849 (20.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# variable length input integer sequences\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer to `embedding_dim` dimensional vector space\n",
    "x = layers.Embedding(vocabulary_size, embedding_dim)(inputs)\n",
    "# Add 1 LSTM layer\n",
    "x = layers.LSTM(64)(x)\n",
    "# Add a classifier (sigmoid activation function for binary classification)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "one_directional_model = keras.Model(inputs, outputs)\n",
    "one_directional_model.summary()"
   ],
   "metadata": {
    "id": "a44817b16d6de233",
    "outputId": "00fe9065-4858-47eb-cd46-3a7fa63da0e8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:14.500091Z",
     "start_time": "2024-11-20T16:05:14.132617Z"
    }
   },
   "id": "a44817b16d6de233",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: The previous model has a huge number of trainable parameters (5.4M) due to the embedding layer (even though the embedding and the vocabulary sizes are not very large). It consumes lots of memory, takes a long time to train, and it requires lots of data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "441b651e253eb8d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compile, train, and evaluate the model. We use the Adam optimizer and the binary cross-entropy loss function. We use early stopping to avoid overfitting.\n",
    "\n",
    "We save the model to avoid retraining it every time we run the notebook."
   ],
   "metadata": {
    "collapsed": false,
    "id": "46fa539415841cc8"
   },
   "id": "46fa539415841cc8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 9s 195ms/step - loss: 0.6946 - accuracy: 0.5040 - val_loss: 0.6925 - val_accuracy: 0.5440\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 6s 175ms/step - loss: 0.5848 - accuracy: 0.7580 - val_loss: 0.7401 - val_accuracy: 0.5760\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 6s 178ms/step - loss: 0.2457 - accuracy: 0.9140 - val_loss: 0.8110 - val_accuracy: 0.5840\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.6963 - accuracy: 0.5280\n",
      "Test loss: 0.6963.\n",
      "Test accuracy: 0.5280.\n"
     ]
    }
   ],
   "source": [
    "def compile_train_evaluate(model_p: keras.Model, x_train_p: np.array, y_train_p: np.array,\n",
    "                           x_val_p: np.array, y_val_p: np.array, x_test_p: np.array, y_test_p: np.array,\n",
    "                           batch_size: int, epochs: int) -> (float, float, keras.Model):\n",
    "    \"\"\"\n",
    "    Compile, train and evaluate the model.\n",
    "    :param model_p: the model to compile, train and evaluate\n",
    "    :param x_train_p: train X sequences\n",
    "    :param y_train_p: train y labels\n",
    "    :param x_val_p: validation X sequences\n",
    "    :param y_val_p: validation y labels\n",
    "    :param x_test_p: test X sequences\n",
    "    :param y_test_p: test y labels\n",
    "    :param batch_size: batch size\n",
    "    :param epochs: number of epochs\n",
    "    :return: (test_loss, test_accuracy, model)\n",
    "    \"\"\"\n",
    "    # we compile and train the model \n",
    "    model_p.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "    model_p.fit(x_train_p, y_train_p, batch_size=batch_size, epochs=epochs, validation_data=(x_val_p, y_val_p),\n",
    "                callbacks=[early_stopping_callback])\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model_p.evaluate(x_test_p, y_test_p)\n",
    "    return loss, accuracy, model_p\n",
    "\n",
    "\n",
    "test_loss, test_accuracy, one_directional_model = compile_train_evaluate(one_directional_model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                       32, n_epochs)\n",
    "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "id": "a10223fcc7637cb3",
    "outputId": "a67572fa-c4cb-4017-acfa-cc348d4ff0b2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T16:05:35.551736Z",
     "start_time": "2024-11-20T16:05:14.504056Z"
    }
   },
   "id": "a10223fcc7637cb3",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bidirectional LSTM model\n",
    "\n",
    "We create a bidirectional LSTM model. We use the same hyperparameters as in the previous model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9e238be6c403d09f"
   },
   "id": "9e238be6c403d09f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 1024)        5120000   \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, 128)               557568    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5677697 (21.66 MB)\n",
      "Trainable params: 5677697 (21.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 21s 514ms/step - loss: 0.6869 - accuracy: 0.5390 - val_loss: 0.6400 - val_accuracy: 0.6320\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 18s 575ms/step - loss: 0.4331 - accuracy: 0.7980 - val_loss: 0.6339 - val_accuracy: 0.6580\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 20s 614ms/step - loss: 0.1245 - accuracy: 0.9660 - val_loss: 0.8602 - val_accuracy: 0.6680\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 20s 636ms/step - loss: 0.0390 - accuracy: 0.9900 - val_loss: 1.1034 - val_accuracy: 0.6660\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.6020 - accuracy: 0.6880\n",
      "Test loss: 0.6020.\n",
      "Test accuracy: 0.6880.\n"
     ]
    }
   ],
   "source": [
    "# variable length input integer sequences\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer to `embedding_dim` dimensional vector space\n",
    "x = layers.Embedding(vocabulary_size, embedding_dim)(inputs)\n",
    "# Add 1 Bidirectional-LSTM layers\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "bi_lstm_model = keras.Model(inputs, outputs)\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "test_loss, test_accuracy, bi_lstm_model = compile_train_evaluate(bi_lstm_model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                                                  32, n_epochs)\n",
    "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "id": "9ef6db743cd17de",
    "outputId": "a6a186d9-73cd-4741-f809-992b76294659",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T16:06:59.367650Z",
     "start_time": "2024-11-20T16:05:35.552988Z"
    }
   },
   "id": "9ef6db743cd17de",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "As in the first model, the bi-LSTM has more than 5.67M trainable parameters. It consumes lots of memory, takes a long time to train, and it requires lots of data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "938b697fc116ac87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions\n",
    "\n",
    "1. Does the bidirectional LSTM model have a significantly higher number of trainable parameters than the one-directional LSTM model? \n",
    "2. Why?\n",
    "3. Does the bidirectional LSTM model perform better than the one-directional LSTM model? \n",
    "4. Why? "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ef63507e7f6d1f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5461946251899efc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ELMo embeddings\n",
    "\n",
    "In the following RNN, we use [ELMo embeddings](https://en.wikipedia.org/wiki/ELMo). ELMo embeddings are context-dependent embeddings, pretrained and ready to use. ELMo embeddins have 1024 dimensions. BERT, and GPT are more powerful embeddings, but require more memory an CPU resources. Nowadays, the most powerful embeddings (e.g., `SentencePiece`) are for subword units rather than words; we use word embeddings for simplicity. "
   ],
   "metadata": {
    "collapsed": false,
    "id": "bf82ca368b2f1c52"
   },
   "id": "bf82ca368b2f1c52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "\n",
    "def get_elmo_embeddings(sentences: list[str]) -> np.array:\n",
    "    \"\"\"\n",
    "    Get ELMo embeddings for a list of sentences.\n",
    "    \"\"\"\n",
    "    # ELMo returns a tensor, but we want to extract the embeddings\n",
    "    embeddings = elmo.signatures['default'](tf.constant(sentences))['elmo']\n",
    "    return embeddings.numpy()  # Convert to numpy array for easier manipulation"
   ],
   "metadata": {
    "id": "525d4f658c233ac6",
    "outputId": "10f39f38-4faa-44e5-b821-811608687ee8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "end_time": "2024-11-20T16:07:05.638058Z",
     "start_time": "2024-11-20T16:06:59.369804Z"
    }
   },
   "id": "525d4f658c233ac6",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have to generate the ELMo embeddings from the reviews. We use the `get_elmo_embeddings` function to get the embeddings for the training, validation, and test sets. This might take time and consume a lot of memory resources (change the value of `max_sentences_train`, `vocabulary_size` and `max_review_size` if you get an out-of-memory error)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aeb4a9f30ff0703"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_elmo = get_elmo_embeddings([decode_review(review) for review in X_train])\n",
    "X_val_elmo = get_elmo_embeddings([decode_review(review) for review in X_val])\n",
    "X_test_elmo = get_elmo_embeddings([decode_review(review) for review in X_test])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T16:09:23.894554Z",
     "start_time": "2024-11-20T16:07:05.641837Z"
    }
   },
   "id": "61c625faef7125ac",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's create a new RNN. *Notice* that we do not need an embedding layer because we are using ELMo embeddings as an input. This makes the model to have fewer parameters. We can use a more complex model with fewer parameters.\n",
    "\n",
    "**Important**: the first RNN layer uses *dropout*. We also define a dropout layer after the LSTM layer. Dropout is a regularization technique that helps to avoid overfitting. I works the following way. In each iteration, some neurons are randomly set to zero. This forces the network to learn more robust features. Dropout is only used during training, not during inference. Dropout is a powerful technique to avoid overfitting in deep learning, but it slows down the training process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5817f11d940f23c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, None, 1024)]      0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, None, 128)         557568    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 656513 (2.50 MB)\n",
      "Trainable params: 656513 (2.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None, embedding_dim), dtype=\"float32\")\n",
    "# Add 2 LSTM layers\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2))(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Dropout layer with 20% rate\n",
    "x = layers.Dropout(0.2)(x)\n",
    "# Add a classifier (sigmoid activation function for binary classification)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "elmo_model = keras.Model(inputs, outputs)\n",
    "elmo_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T16:09:25.204298Z",
     "start_time": "2024-11-20T16:09:23.905188Z"
    }
   },
   "id": "2232a176127a5b91",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: This model has 656K trainable parameters, which is much less than the previous models (11.5% the parameters of the previous RNN network). It consumes less memory, takes less time to train, and requires fewer data.\n",
    "\n",
    "We compile, train, and evaluate the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fffd59a5f21504b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 24s 505ms/step - loss: 0.2129 - accuracy: 0.9150 - val_loss: 0.7469 - val_accuracy: 0.7240\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 19s 597ms/step - loss: 0.1095 - accuracy: 0.9660 - val_loss: 0.9210 - val_accuracy: 0.7040\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 20s 640ms/step - loss: 0.0521 - accuracy: 0.9850 - val_loss: 0.9302 - val_accuracy: 0.7040\n",
      "16/16 [==============================] - 4s 277ms/step - loss: 0.6726 - accuracy: 0.7320\n",
      "Test loss: 0.6726.\n",
      "Test accuracy: 0.7320.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, elmo_model = compile_train_evaluate(elmo_model, X_train_elmo, y_train, X_val_elmo, y_val, X_test_elmo, y_test,\n",
    "                                                              32, n_epochs)\n",
    "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T16:17:32.811235Z",
     "start_time": "2024-11-20T16:16:24.482891Z"
    }
   },
   "id": "9ba60594655fb52c",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "We can see how accurate the mode is by predicting the sentiment of some reviews. What follows are some example reviews. Add more reviews to test the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd6eeea905c22b8d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "Review 1: The movie was a great waste of time. I is awful and boring.\n",
      "Probability of being positive: 0.1262.\n",
      "\n",
      "Review 2: I loved the movie. The plot was amazing.\n",
      "Probability of being positive: 0.9213.\n",
      "\n",
      "Review 3: This movie is not worth watching.\n",
      "Probability of being positive: 0.3480.\n",
      "\n",
      "Review 4: Although the film is not a masterpiece, you may have a good time if your expectations are not high.\n",
      "Probability of being positive: 0.8530.\n"
     ]
    }
   ],
   "source": [
    "example_reviews = [\"The movie was a great waste of time. I is awful and boring.\",\n",
    "                   \"I loved the movie. The plot was amazing.\",\n",
    "                   \"This movie is not worth watching.\",\n",
    "                   \"Although the film is not a masterpiece, you may have a good time if your expectations are not high.\"]\n",
    "\n",
    "\n",
    "review_embeddings = get_elmo_embeddings(example_reviews)\n",
    "predictions = elmo_model.predict(review_embeddings)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Review {i + 1}: {example_reviews[i]}\")\n",
    "    print(f\"Probability of being positive: {prediction[0]:.4f}.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T16:26:21.191764Z",
     "start_time": "2024-11-20T16:26:20.652655Z"
    }
   },
   "id": "e1a42efa2648826b",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions\n",
    "\n",
    "5. After testing some reviews, do you think the model is performing reasonably well? \n",
    "6. Why do you think, then, that the accuracy is not higher (it is 0.7320)? Enumerate as many reasons as you can to see if you understand how it works.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187875f3d90273b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e490b65a2d034e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
