{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/sentiment.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "87540b40e3958a06"
   },
   "id": "87540b40e3958a06"
  },
  {
   "cell_type": "markdown",
   "id": "72ea15f3d70547f7",
   "metadata": {
    "collapsed": false,
    "id": "72ea15f3d70547f7"
   },
   "source": [
    "# Sentiment classification with RNNs\n",
    "\n",
    "In this notebook, we implement a sentiment analysis task to classify movie reviews as positive or negative. By analyzing the text of written by the users, we will predict the sentiment of each review. We use the the [IMDb dataset](https://keras.io/api/datasets/imdb/), a set of 50,000 movie reviews from the [Internet Movie Database](https://en.wikipedia.org/wiki/IMDb) (IMDb).\n",
    "\n",
    "<img src=\"img/imdb.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f099c66705669f8",
   "metadata": {
    "id": "5f099c66705669f8",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:22.705763Z",
     "start_time": "2024-11-19T14:38:19.718617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/rnn'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "    !cp {directory}/img/* img/.\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3e8a3ff34b7dc",
   "metadata": {
    "collapsed": false,
    "id": "e7c3e8a3ff34b7dc"
   },
   "source": [
    "## Load the IMDb dataset\n",
    "\n",
    " We use the 400,000 most frequent words in the dataset. We cut the reviews to a maximum length of 80 words to speed up training. We use embeddings of 50 dimensions and a large number of epochs (50) to train the models because we use early stopping."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# consider all the words with a frequency higher than this value\n",
    "vocabulary_size = 400_000\n",
    "# compute the maximum length of the reviews (for speeding up the training it is better to cut the reviews)\n",
    "max_review_length = 80\n",
    "# max number of epochs to train the models (we use early stopping)\n",
    "n_epochs = 50\n",
    "# Embedding dimensions\n",
    "embedding_dim = 50"
   ],
   "metadata": {
    "id": "f5f7eb0c67d58ee5",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:22.719364Z",
     "start_time": "2024-11-19T14:38:22.711376Z"
    }
   },
   "id": "f5f7eb0c67d58ee5",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the dataset from the Keras API. Half of the reviews are used for training, and the other half for validation and testing."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c50a52ab52105e74"
   },
   "id": "c50a52ab52105e74"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: 25,000.\n",
      "Validation sequences: 12,500.\n",
      "Testing sequences: 12,500.\n"
     ]
    }
   ],
   "source": [
    "# we train one half for training and the other half for validation and testing\n",
    "(X_train, y_train), (x_half, y_half) = keras.datasets.imdb.load_data(num_words=vocabulary_size)\n",
    "# get validation set as half of the test set\n",
    "X_test, X_val = x_half[:len(x_half) // 2], x_half[len(x_half) // 2:]\n",
    "y_test, y_val = y_half[:len(y_half) // 2], y_half[len(y_half) // 2:]\n",
    "\n",
    "print(f\"Training sequences: {len(X_train):,}.\\nValidation sequences: {len(X_val):,}.\\nTesting sequences: {len(X_test):,}.\")"
   ],
   "metadata": {
    "id": "73a4ffd175d95c8b",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:25.465341Z",
     "start_time": "2024-11-19T14:38:22.722749Z"
    }
   },
   "id": "73a4ffd175d95c8b",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process the dataset\n",
    "\n",
    "We store all the word indexes returned by the IMDb dataset in a dictionary. An `index_to_word` dictionary is created to convert the token IDs back to words. We reserve the first four indices for special tokens `<PAD>`, `<START>`, `<OOV>`, and `<END>`."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5f82fcb2a4d25d87"
   },
   "id": "5f82fcb2a4d25d87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's print some reviews. We need to convert the integers (token ids) back to words.\n",
    "word_to_index = {word: index+3 for word, index in keras.datasets.imdb.get_word_index().items()}  # word -> integer dictionary\n",
    "# The IMDB dataset reserves the 4 first indices for special tokens <PAD>, <START>, <OOV>, <END>\n",
    "index_to_word = {value: key for key, value in word_to_index.items()}  # integer -> word dictionary\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<START>\"\n",
    "index_to_word[2] = \"<OOV>\"\n",
    "index_to_word[3] = \"<END>\""
   ],
   "metadata": {
    "id": "c98ebd5c105cfde3",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:25.576192Z",
     "start_time": "2024-11-19T14:38:25.470238Z"
    }
   },
   "id": "c98ebd5c105cfde3",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "We show the first reviews and their corresponding sentiment. "
   ],
   "metadata": {
    "collapsed": false,
    "id": "d564187c851e6ad8"
   },
   "id": "d564187c851e6ad8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First reviews in training set, with the corresponding labels:\n",
      "Review 1: <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all.\n",
      "Label: 1.\n",
      "Review 2: <START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal measures the hair is big lots of boobs bounce men wear those cut tee shirts that show off their stomachs sickening that men actually wore them and the music is just synthesiser trash that plays over and over again in almost every scene there is trashy music boobs and paramedics taking away bodies and the gym still doesn't close for bereavement all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then.\n",
      "Label: 0.\n",
      "Review 3: <START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had earnt working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how embarrasing this is to watch save yourself an hour a bit of your life.\n",
      "Label: 0.\n",
      "Review 4: <START> the scots excel at storytelling the traditional sort many years after the event i can still see in my mind's eye an elderly lady my friend's mother retelling the battle of culloden she makes the characters come alive her passion is that of an eye witness one to the events on the sodden heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and breadth of scotland as i discussed it with a friend one night in mallaig a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional reservoir fact and fiction blend with aspiration role models warning stories archetypes magic and mystery br br my name is aonghas like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the impenetrable wonder of scotland its rugged mountains shrouded in mists the stuff of legend yet seach'd is rooted in reality this is what gives it its special charm it has a rough beauty and authenticity tempered with some of the finest gaelic singing you will ever hear br br aonghas angus visits his grandfather in hospital shortly before his death he burns with frustration part of him yearns to be in the twenty first century to hang out in glasgow but he is raised on the western shores among a gaelic speaking community br br yet there is a deeper conflict within him he yearns to know the truth the truth behind his grandfather's ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last fateful journey to the summit of one of scotland's most inaccessible mountains can the truth be told or is it all in stories br br in this story about stories we revisit bloody battles poisoned lovers the folklore of old and the sometimes more treacherous folklore of accepted truth in doing so we each connect with angus as he lives the story of his own life br br seachd the inaccessible pinnacle is probably the most honest unpretentious and genuinely beautiful film of scotland ever made like angus i got slightly annoyed with the pretext of hanging stories on more stories but also like angus i forgave this once i saw the 'bigger picture ' forget the box office pastiche of braveheart and its like you might even forego the justly famous dramatisation of the wicker man to see a film that is true to scotland this one is probably unique if you maybe meditate on it deeply enough you might even re evaluate the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced.\n",
      "Label: 1.\n",
      "Review 5: <START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the microwave and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life.\n",
      "Label: 0.\n"
     ]
    }
   ],
   "source": [
    "def decode_review(encoded_review: list[int]) -> str:\n",
    "    \"\"\"Decode a review from a list of integers to a string.\"\"\"\n",
    "    return ' '.join(index_to_word.get(word_index, \"<OOV>\") for word_index in encoded_review)\n",
    "\n",
    "print(\"First reviews in training set, with the corresponding labels:\")\n",
    "for (i, (review, label)) in enumerate(zip(X_train[:5], y_train[:5])):\n",
    "    print(f\"Review {i + 1}: {decode_review(review)}.\\nLabel: {label}.\")"
   ],
   "metadata": {
    "id": "43ef1964ad0dad2",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:25.592289Z",
     "start_time": "2024-11-19T14:38:25.581237Z"
    }
   },
   "id": "43ef1964ad0dad2",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add padding to the reviews to have the same length. We use the `post` mode to pad and truncate at the end of the reviews."
   ],
   "metadata": {
    "collapsed": false,
    "id": "be41a096e9dbefa9"
   },
   "id": "be41a096e9dbefa9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pad the reviews to have the same length (padding and truncating at the end with \"post\")\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length, padding=\"post\", truncating=\"post\")"
   ],
   "metadata": {
    "id": "419ca03571ca5b6d",
    "outputId": "36540554-d259-4ae4-d30c-e76419c7708a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:25.909625Z",
     "start_time": "2024-11-19T14:38:25.596290Z"
    }
   },
   "id": "419ca03571ca5b6d",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-directional LSTM model\n",
    "\n",
    "We create a one-direction LSTM RNN where embeddings are computed as the first layer using the Keras `Embedding` layer. We have to choose the size of the embedding vectors (hyperparameter)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2c21cb3cd6d19e51"
   },
   "id": "2c21cb3cd6d19e51"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 50)          20000000  \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                29440     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20029505 (76.41 MB)\n",
      "Trainable params: 20029505 (76.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# variable length input integer sequences\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer to `embedding_dim` dimensional vector space\n",
    "x = layers.Embedding(vocabulary_size, embedding_dim)(inputs)\n",
    "# Add 1 LSTM layer\n",
    "x = layers.LSTM(64)(x)\n",
    "# Add a classifier (sigmoid activation function for binary classification)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "one_directional_model = keras.Model(inputs, outputs)\n",
    "one_directional_model.summary()"
   ],
   "metadata": {
    "id": "a44817b16d6de233",
    "outputId": "00fe9065-4858-47eb-cd46-3a7fa63da0e8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:26.605473Z",
     "start_time": "2024-11-19T14:38:25.913696Z"
    }
   },
   "id": "a44817b16d6de233",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: The previous model has a huge number of trainable parameters (20M) due to the embedding layer (even though the embedding size is not very high, but the vocabulary size is very large). It consumes lots of memory, takes a long time to train, and it requires lots of data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "441b651e253eb8d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compile, train, and evaluate the model. We use the Adam optimizer and the binary cross-entropy loss function. We use early stopping to avoid overfitting.\n",
    "\n",
    "We save the model to avoid retraining it every time we run the notebook."
   ],
   "metadata": {
    "collapsed": false,
    "id": "46fa539415841cc8"
   },
   "id": "46fa539415841cc8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 9s 21ms/step - loss: 0.6025 - accuracy: 0.6826\n",
      "Test loss: 0.6025.\n",
      "Test accuracy: 0.6826.\n"
     ]
    }
   ],
   "source": [
    "def compile_train_evaluate(model_p: keras.Model, x_train_p: np.array, y_train_p: np.array,\n",
    "                           x_val_p: np.array, y_val_p: np.array, x_test_p: np.array, y_test_p: np.array,\n",
    "                           batch_size: int, epochs: int, zip_file_name: str, model_file_name: str) -> (float, float, keras.Model):\n",
    "    \"\"\"\n",
    "    Compile, train and evaluate the model.\n",
    "    :param model_p: the model to compile, train and evaluate\n",
    "    :param x_train_p: train X sequences\n",
    "    :param y_train_p: train y labels\n",
    "    :param x_val_p: validation X sequences\n",
    "    :param y_val_p: validation y labels\n",
    "    :param x_test_p: test X sequences\n",
    "    :param y_test_p: test y labels\n",
    "    :param batch_size: batch size\n",
    "    :param epochs: number of epochs\n",
    "    :param zip_file_name: file name to store/load the model compressed\n",
    "    :param model_file_name: file name to inside the zip file\n",
    "    :return: (test_loss, test_accuracy, model)\n",
    "    \"\"\"\n",
    "    # we compile and train the model if it does not exist (otherwise we load it)\n",
    "    if not os.path.exists(zip_file_name):\n",
    "        model_p.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "        model_p.fit(x_train_p, y_train_p, batch_size=batch_size, epochs=epochs, validation_data=(x_val_p, y_val_p),\n",
    "                    callbacks=[early_stopping_callback])\n",
    "        # save the model\n",
    "        model_p.save(model_file_name)\n",
    "        # compress the model file\n",
    "        with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.write(model_file_name, arcname=model_file_name)\n",
    "        # remove the model file\n",
    "        os.remove(model_file_name)\n",
    "    else:\n",
    "        # load the model; open the zip file and extract the model file\n",
    "        with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        # load the model\n",
    "        model_p = keras.models.load_model(model_file_name)\n",
    "        # remove the model file\n",
    "        os.remove(model_file_name)\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model_p.evaluate(x_test_p, y_test_p)\n",
    "    return loss, accuracy, model_p\n",
    "\n",
    "\n",
    "test_loss, test_accuracy, one_directional_model = compile_train_evaluate(one_directional_model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                       32, n_epochs, \"data/one_directional_model.zip\", 'one_directional_model.keras')\n",
    "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "id": "a10223fcc7637cb3",
    "outputId": "a67572fa-c4cb-4017-acfa-cc348d4ff0b2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T14:38:43.130384Z",
     "start_time": "2024-11-19T14:38:26.609621Z"
    }
   },
   "id": "a10223fcc7637cb3",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bidirectional LSTM model\n",
    "\n",
    "We create a bidirectional LSTM model. We use the same hyperparameters as in the previous model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9e238be6c403d09f"
   },
   "id": "9e238be6c403d09f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, None, 50)          20000000  \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 128)               58880     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20059009 (76.52 MB)\n",
      "Trainable params: 20059009 (76.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "391/391 [==============================] - 8s 16ms/step - loss: 0.5194 - accuracy: 0.7508\n",
      "Test loss: 0.5194.\n",
      "Test accuracy: 0.7508.\n"
     ]
    }
   ],
   "source": [
    "# variable length input integer sequences\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer to `embedding_dim` dimensional vector space\n",
    "x = layers.Embedding(vocabulary_size, embedding_dim)(inputs)\n",
    "# Add 1 Bidirectional-LSTM layers\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "bi_lstm_model = keras.Model(inputs, outputs)\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "test_loss, test_accuracy, bi_lstm_model = compile_train_evaluate(bi_lstm_model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                                                  32, n_epochs, \"data/bi_directional_model.zip\", \"bi_directional_model.keras\")\n",
    "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "id": "9ef6db743cd17de",
    "outputId": "a6a186d9-73cd-4741-f809-992b76294659",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T14:39:01.176839Z",
     "start_time": "2024-11-19T14:38:43.131573Z"
    }
   },
   "id": "9ef6db743cd17de",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "As in the first model, the bi-LSTM has more than 20M trainable parameters. It consumes lots of memory, takes a long time to train, and it requires lots of data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "938b697fc116ac87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions\n",
    "\n",
    "1. Does the bidirectional LSTM model have a significantly higher number of trainable parameters than the one-directional LSTM model? \n",
    "2. Why?\n",
    "3. Does the bidirectional LSTM model perform better than the one-directional LSTM model? \n",
    "4. Why? "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ef63507e7f6d1f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5461946251899efc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GloVe embeddings\n",
    "\n",
    "In the following RNN, we use [GloVe embeddings](https://nlp.stanford.edu/projects/glove/). GloVe embeddings are 400,000 pre-trained word vectors that can be used in any NLP task. They are not context-dependent, but they are useful when the training data is limited. ELMo, BERT, and GPT are context-dependent embeddings that are more powerful but require more memory an CPU resources. Nowadays, the most powerful embeddings (e.g., `SentencePiece`) are for subword units rather than words; we use word embeddings for simplicity. \n",
    "\n",
    "We use the 50-dimensional GloVe embeddings to create a dictionary with the embeddings. They are the least computationally expensive embeddings available: GloVe embeddings are also provided with 100, 200, and 300 dimensions. The last one requires more than 1 GB.\n",
    "\n",
    "We load the GloVe embeddings from the file `glove.6B.50d.zip` and create a dictionary with the embeddings."
   ],
   "metadata": {
    "collapsed": false,
    "id": "bf82ca368b2f1c52"
   },
   "id": "bf82ca368b2f1c52"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,003 word embeddings of 50 dimensions.\n"
     ]
    }
   ],
   "source": [
    "def create_glove_embeddings_from_file(zip_file_name: str, txt_file_name: str) -> dict[str, np.array]:\n",
    "    \"\"\"\n",
    "    Create a dictionary of GloVe embeddings from a file.\n",
    "    :param zip_file_name: the zip file name\n",
    "    :param txt_file_name: the text file name inside the zip file\n",
    "    :return: the dictionary of embeddings\n",
    "    \"\"\"\n",
    "    glove_embeddings_loc = {}  # word -> vector(embedding_dim) mapping\n",
    "    with zipfile.ZipFile(zip_file_name, 'r') as zip_file:\n",
    "        with zip_file.open(txt_file_name, 'r') as file:\n",
    "            # load the vocabulary_size most frequent words, including padding, start and OOV tokens\n",
    "            for line in file:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                glove_embeddings_loc[word] = vector\n",
    "            glove_embeddings_loc[\"<PAD>\"] = np.zeros(embedding_dim)\n",
    "            glove_embeddings_loc[\"<START>\"] = np.full(embedding_dim, 0.5)\n",
    "            glove_embeddings_loc[\"<OOV>\"] = np.ones(embedding_dim)\n",
    "    return glove_embeddings_loc\n",
    "\n",
    "\n",
    "glove_embeddings = create_glove_embeddings_from_file(\"data/glove.6B.50d.zip\", \"glove.6B.50d.txt\")\n",
    "print(f\"Found {len(glove_embeddings):,} word embeddings of {embedding_dim} dimensions.\")"
   ],
   "metadata": {
    "id": "525d4f658c233ac6",
    "outputId": "10f39f38-4faa-44e5-b821-811608687ee8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T14:39:07.403553Z",
     "start_time": "2024-11-19T14:39:01.179559Z"
    }
   },
   "id": "525d4f658c233ac6",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a matrix with the GloVe embeddings for the words in the IMDb dataset. That matrix will be used in the following RNN as the embedding layer. To fix it, we iterate over the `vocabulary_sizze` words in the dataset, take the word and search for its embedding in the GloVe embeddings (OOV embedding if the word not found).\n",
    "\n",
    "We use the embeddings for the OOV token if the word is not found."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1aeb4a9f30ff0703"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_glove_word_embedding(word_p: str, glove_embeddings_p: dict[str, np.array]) -> np.array:\n",
    "    \"\"\"\n",
    "    Get the GloVe embedding for a word. It is not found, return the embedding for the OOV token.\n",
    "    \"\"\"\n",
    "    return glove_embeddings.get(word_p, glove_embeddings_p[\"<OOV>\"])\n",
    "\n",
    "\n",
    "glove_embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "for word_index in range(vocabulary_size):\n",
    "    word = index_to_word.get(word_index, \"<OOV>\")\n",
    "    embedding_vector = get_glove_word_embedding(word, glove_embeddings)\n",
    "    glove_embedding_matrix[word_index] = embedding_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T14:39:07.774960Z",
     "start_time": "2024-11-19T14:39:07.408408Z"
    }
   },
   "id": "61c625faef7125ac",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's create a new RNN with the GloVe embeddings. **Notice** that the embedding layer is not trainable. We use the GloVe embeddings as they are."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5817f11d940f23c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, None, 50)          20000000  \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, None, 128)         58880     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirecti  (None, 128)               98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20157825 (76.90 MB)\n",
      "Trainable params: 157825 (616.50 KB)\n",
      "Non-trainable params: 20000000 (76.29 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# We set the matrix weights to the GloVe embeddings (`weights`) and we do not train the embeddings (`trainable=False`)\n",
    "x = layers.Embedding(vocabulary_size, embedding_dim, weights=[glove_embedding_matrix], trainable=False)(inputs)\n",
    "# Add 2 Bidirectional-LSTM layers\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "glove_lstm_model = keras.Model(inputs, outputs)\n",
    "glove_lstm_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T14:39:09.582210Z",
     "start_time": "2024-11-19T14:39:07.779883Z"
    }
   },
   "id": "2232a176127a5b91",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: This model has 157,825 trainable parameters, which is much less than the previous models (0.78% the parameters of the two previous RNN networks). It consumes less memory, takes less time to train, and requires fewer data.\n",
    "\n",
    "We compile, train, and evaluate the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fffd59a5f21504b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 14s 30ms/step - loss: 0.4449 - accuracy: 0.7890\n",
      "Test loss: 0.4449.\n",
      "Test accuracy: 0.7890.\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, glove_lstm_model = compile_train_evaluate(glove_lstm_model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                                                 32, n_epochs, \"data/glove_model.zip\", \"glove_model.keras\")\n",
    "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T14:39:28.680124Z",
     "start_time": "2024-11-19T14:39:09.585589Z"
    }
   },
   "id": "9ba60594655fb52c",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "We can see how accurate the mode is by predicting the sentiment of some reviews. What follows are some example reviews. Add more reviews to test the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd6eeea905c22b8d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Review 1: The movie was a great waste of time. The plot was boring.\n",
      "Probability of being positive: 0.0108.\n",
      "\n",
      "Review 2: I loved the movie. The plot was amazing.\n",
      "Probability of being positive: 0.9680.\n",
      "\n",
      "Review 3: Would not recommend this movie to anyone.\n",
      "Probability of being positive: 0.4301.\n",
      "\n",
      "Review 4: The movie is not a masterpiece. You may have a good time if your expectations are not high.\n",
      "Probability of being positive: 0.6280.\n"
     ]
    }
   ],
   "source": [
    "example_reviews = [\"The movie was a great waste of time. The plot was boring.\",\n",
    "                   \"I loved the movie. The plot was amazing.\",\n",
    "                   \"Would not recommend this movie to anyone.\",\n",
    "                   \"The movie is not a masterpiece. You may have a good time if your expectations are not high.\"]\n",
    "\n",
    "\n",
    "def prepare_reviews_for_prediction(reviews_p: list[str], word_to_index_p: dict[str, int], max_review_length_p: int)\\\n",
    "        -> np.array:    \n",
    "    \"\"\"\n",
    "    Prepare a list of reviews for prediction: include the <START> token, convert the words to lower case,\n",
    "    remove punctuation, convert the words to indexes, pad the sequences and truncate them if necessary.\n",
    "    :param reviews_p: the list of reviews to be prepared \n",
    "    :param word_to_index_p: a dictionary mapping words to indexes\n",
    "    :param max_review_length_p: the maximum length of the reviews\n",
    "    :return: and array of token sequences, one for each review\n",
    "    \"\"\"\n",
    "    sequences_loc = []\n",
    "    for review in reviews_p:\n",
    "        words_indexes = [1]  # start token\n",
    "        for word in review.split():\n",
    "            for char_to_remove in [\".\", \",\", \"!\", \"?\"]:\n",
    "                word = word.lower().replace(char_to_remove, \"\")\n",
    "            words_indexes.append(word_to_index_p[word] if word in word_to_index_p else 2)  # OOV token\n",
    "        sequences_loc.append(words_indexes)  \n",
    "    # pad the sequences\n",
    "    sequences_loc = keras.preprocessing.sequence.pad_sequences(sequences_loc, maxlen=max_review_length_p, \n",
    "                                                               padding=\"post\", truncating=\"post\")\n",
    "    return sequences_loc\n",
    "\n",
    "\n",
    "sequences_to_predict = prepare_reviews_for_prediction(example_reviews, word_to_index, max_review_length)\n",
    "predictions = glove_lstm_model.predict(sequences_to_predict)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Review {i + 1}: {example_reviews[i]}\")\n",
    "    print(f\"Probability of being positive: {prediction[0]:.4f}.\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T14:39:30.545094Z",
     "start_time": "2024-11-19T14:39:28.681242Z"
    }
   },
   "id": "e1a42efa2648826b",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions\n",
    "\n",
    "5. After testing some reviews, do yo think the model is performing reasonably well? \n",
    "6. Why do you think, then, that the accuracy is not higher (it is 0.7890)?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187875f3d90273b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e490b65a2d034e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
