{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/simple_rnn.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87540b40e3958a06"
  },
  {
   "cell_type": "markdown",
   "id": "72ea15f3d70547f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simple Recunrrent Neural Networks (RNNs) for sequence prediction\n",
    "\n",
    "TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f099c66705669f8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T17:02:40.303992500Z",
     "start_time": "2024-10-11T17:02:15.962273200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "WARNING:tensorflow:From C:\\Users\\ortin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/rnn'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3e8a3ff34b7dc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data preparation\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## LOAD THE DATA\n",
    "\n",
    "dataset = pd.read_csv('data/transit.csv', parse_dates=['service_date'])  # parse_dates converts the column to datetime\n",
    "# we will use the number of passengers for the train as the target variable, so bus and total_rides columns are not necessary\n",
    "dataset = dataset.drop(columns=['bus', 'total_rides'])\n",
    "# rename the rail_boardings column to rail and the service_date column to date\n",
    "dataset = dataset.rename(columns={\"rail_boardings\": \"rail\", \"service_date\": \"date\"})\n",
    "# remove duplicates\n",
    "dataset = dataset.drop_duplicates()\n",
    "# sort by date (ascending) and set the date as the index of the dataframe\n",
    "dataset = dataset.sort_values(\"date\").set_index(\"date\")\n",
    "# display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(dataset.head())\n",
    "\n",
    "\n",
    "# Let's plot the number of train passengers for the first semester months of 2001\n",
    "#dataset[\"2001-01\":\"2001-06\"].plot(grid=True, marker=\".\", figsize=(12, 3.5))\n",
    "#plt.show()\n",
    "\n",
    "# The default activation function for the RNN is the hyperbolic tangent (tanh) function,\n",
    "# which outputs values between -1 and 1. A simple RNN uses the output as part of the input for the next time step.\n",
    "# Therefore, we need to scale the data to be between -1 and 1. We use the MinMaxScaler from scikit-learn to scale the data.\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "dataset['rail'] = scaler.fit_transform(dataset[['rail']])\n",
    "# convert the date_type column to a one-hot encoded column\n",
    "dataset = pd.get_dummies(dataset, columns=[\"day_type\"])\n",
    "# map True to 1 and False to 0\n",
    "dataset = dataset.astype({\"day_type_A\": int, \"day_type_U\": int, \"day_type_W\": int})\n",
    "print(\"First few rows of the dataset after scaling and one-hot encoding:\")\n",
    "print(dataset.head())\n",
    "\n",
    "## SPLIT THE DATA INTO TRAINING, VALIDATION AND TESTING SETS\n",
    "\n",
    "train_ds = dataset[:\"2015-12\"]\n",
    "val_ds = dataset[\"2016-01\":\"2018-12\"]\n",
    "test_ds = dataset[\"2019-01\":]\n",
    "\n",
    "def create_variable_time_series_dataset(dataset_p: pd.DataFrame, length_from_p: int, length_to_p: int) \\\n",
    "        -> (list, list):\n",
    "    \"\"\"\n",
    "    Create a variable length time series dataset\n",
    "    :param dataset_p: the original dataset with the time series\n",
    "    :param length_from_p: the minimum length of the time series X set\n",
    "    :param length_to_p: the maximum length of the time series X set\n",
    "    :return: (X, y) where X is the time series and y is the target variable\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(dataset_p)):\n",
    "        series_length = np.random.randint(length_from_p, length_to_p + 1)\n",
    "        if i + series_length < len(dataset_p):\n",
    "            X.append(np.array(dataset_p.iloc[i:i + series_length][['rail', 'day_type_A', 'day_type_U', 'day_type_W']].values))\n",
    "            y.append(dataset_p.iloc[i + series_length]['rail'])\n",
    "        else:\n",
    "            break\n",
    "    return X, y\n",
    "\n",
    "\n",
    "length_from, length_to = 40, 60\n",
    "X_train, y_train = create_variable_time_series_dataset(train_ds, length_from, length_to)\n",
    "X_val, y_val = create_variable_time_series_dataset(val_ds, length_from, length_to)\n",
    "X_test, y_test = create_variable_time_series_dataset(test_ds, length_from, length_to)\n",
    "\n",
    "\n",
    "# Select sequences of random length between 30 and 60 days using tf.keras.utils.timeseries_dataset_from_array\n",
    "\n",
    "\n",
    "def X_y_to_tensor_slices(X_p: list, y_p: list) -> tf.data.Dataset:\n",
    "    # Convert to TensorFlow Dataset\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: zip(X_p, y_p),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 4), dtype=tf.float32),  # X has shape [None, 5]: None for variable sequence length, and 5 for the number of features in each timestep\n",
    "            tf.TensorSpec(shape=(), dtype=tf.float32)          # y is a scalar\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_set_tf = X_y_to_tensor_slices(X_train, y_train)\n",
    "val_set_tf = X_y_to_tensor_slices(X_val, y_val)\n",
    "test_set_tf = X_y_to_tensor_slices(X_test, y_test)\n",
    "\n",
    "# Batch each sequence individually (batch of 1) to allow variable-length input\n",
    "# (mandatory for training RNNs with variable-length sequences (not very common)).\n",
    "# At inference, any length can be used, even though the model was trained with a specific length.\n",
    "train_set_tf = (train_set_tf.batch(1)  # organizes the dataset into batches:\n",
    "                # adds a new dimension batch-size to the dataset, grouping elements into batches\n",
    "                # batch size is 1 to allow for variable length sequences\n",
    "                .prefetch(1))  # prefetches the next batch while training on the current batch (pipeline parallelism)\n",
    "                        # it is set to 1 to prefetch 1 batch while training on another 1 batch\n",
    "\n",
    "val_set_tf = val_set_tf.batch(1).prefetch(1)\n",
    "test_set_tf = test_set_tf.batch(1).prefetch(1)\n",
    "\n",
    "# Define the model\n",
    "simple_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(1, input_shape=[None, 4])  # None = variable length sequence, 4 = number of features in each timestep\n",
    "])\n",
    "print(simple_model.summary())\n",
    "\n",
    "def train_model(model: Model, train_set_p: tf.data.Dataset, val_set_p: tf.data.Dataset, learning_rate: float, epochs: int) \\\n",
    "        -> tf.keras.callbacks.History:\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    # huber loss is less is useful for time series data\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae', 'mape'])\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_mae\", patience=5, restore_best_weights=True)\n",
    "    history = model.fit(train_set_p, validation_data=val_set_p, epochs=epochs,\n",
    "                        batch_size=1, callbacks=[early_stopping_cb])\n",
    "    return history\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "train_model(simple_model, train_set_tf, val_set_tf, learning_rate=0.01, epochs=epochs)\n",
    "test_loss, test_mae, test_mape = simple_model.evaluate(test_set_tf)\n",
    "# The test MAE is scaled, so we need to inverse the scaling to get the actual value.\n",
    "# Since the output is a 2d array (bath_size, y features), we need to select the first element of the first element of the array.\n",
    "print(f\"Test MAE: {scaler.inverse_transform([[test_mae]])[0][0]:.0f}.\")\n",
    "print(f\"Test MAPE: {test_mape:.2f}.\")\n",
    "\n",
    "\n",
    "def plot_predictions(y_test_p: np.array, y_pred_p: np.array, n_instances: int):\n",
    "    \"\"\"\n",
    "    Plot the first 100 instances of y_pred vs y_test\n",
    "    :param y_test_p: the true values\n",
    "    :param y_pred_p: the predicted values\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 3.5))\n",
    "    plt.plot(y_test_p[:n_instances], label=\"True values\")\n",
    "    plt.plot(y_pred_p[:n_instances], label=\"Predicted values\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "n_instances_to_plot = 100\n",
    "y_pred = simple_model.predict(test_set_tf)\n",
    "plot_predictions(y_test, y_pred, n_instances_to_plot)\n",
    "\n",
    "\n",
    "## MULTIPLE SIMPLE RECURRENT NEURONS\n",
    "\n",
    "# The model is not very accurate, but it is a simple model. We can try to improve the model by adding a layer of simple RNNs.\n",
    "# Instead of one simple recurrent neuron, we can add 32 simple recurrent neurons to the model.\n",
    "# Since the output has to be a scalar, we need to add a Dense layer with one neuron at the end of the model.\n",
    "\n",
    "multi_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 4]),\n",
    "    tf.keras.layers.Dense(1)  # no activation function by default (regression model)\n",
    "])\n",
    "print(multi_model.summary())\n",
    "\n",
    "train_model(multi_model, train_set_tf, val_set_tf, learning_rate=0.01, epochs=epochs)\n",
    "test_loss, test_mae, test_mape = multi_model.evaluate(test_set_tf)\n",
    "print(f\"Test MAE: {scaler.inverse_transform([[test_mae]])[0][0]:.0f}.\")\n",
    "print(f\"Test MAPE: {test_mape:.2f}.\")\n",
    "\n",
    "y_pred = multi_model.predict(test_set_tf)\n",
    "plot_predictions(y_test, y_pred, n_instances_to_plot)\n",
    "\n",
    "\n",
    "## DEEP RNN\n",
    "\n",
    "# We can also try to improve the model by adding more layers to the model.\n",
    "# We can add three layers of simple RNNs with 32 neurons each.\n",
    "# Each layer will return the output of each time step, so they can be stacked.\n",
    "# To that aim we set return_sequences=True.\n",
    "# We can also add a Dense layer with one neuron at the end of the model.\n",
    "\n",
    "\n",
    "deep_model = tf.keras.Sequential([\n",
    "    # return_sequences=True is used to return the output of each time step, not just the last one\n",
    "    # in this way, the next layer can process the output of each time step (they can be stacked)\n",
    "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 4], return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(32, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 4]),\n",
    "    tf.keras.layers.Dense(1)  # no activation function by default (regression model)\n",
    "])\n",
    "print(deep_model.summary())\n",
    "\n",
    "train_model(deep_model, train_set_tf, val_set_tf, learning_rate=0.01, epochs=epochs)\n",
    "test_loss, test_mae, test_mape = deep_model.evaluate(test_set_tf)\n",
    "print(f\"Test MAE: {scaler.inverse_transform([[test_mae]])[0][0]:.0f}.\")\n",
    "print(f\"Test MAPE: {test_mape:.2f}.\")\n",
    "\n",
    "y_pred = deep_model.predict(test_set_tf)\n",
    "plot_predictions(y_test, y_pred, n_instances_to_plot)\n",
    "\n",
    "\n",
    "## QUESTIONS:\n",
    "# 1. Are model preformed being improved by adding more complexity/parameters?\n",
    "# 2. Why?\n",
    "# 3. Do you think that will always be the case?\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f7eb0c67d58ee5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
