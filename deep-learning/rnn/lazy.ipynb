{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/lazy.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "bbf6d0671dafefcc"
   },
   "id": "bbf6d0671dafefcc"
  },
  {
   "cell_type": "markdown",
   "id": "72ea15f3d70547f7",
   "metadata": {
    "collapsed": false,
    "id": "72ea15f3d70547f7"
   },
   "source": [
    "# Training Deep ANNs with lazy upload of data\n",
    "\n",
    "In the previous notebook, we limited the data loaded into memory to avoid memory issues. However, deep models have plenty of parameters to be learned from lots of data. This generates a problem: we need to load the data into memory to train the model, but we cannot load all the data at once.\n",
    "\n",
    "To solve that problem, mini-batches are used to lazily load the data in small chunks. The `fit` method allows us to pass a generator that yields the data in small chunks (mini-batches). In this way, we do not need to load the whole dataset into memory at once. This technique is very important when dealing with large datasets (big data), very common when training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f099c66705669f8",
   "metadata": {
    "id": "5f099c66705669f8",
    "outputId": "46892b24-f56b-48c5-f846-4cbc03cb1958",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:30.302781Z",
     "start_time": "2024-11-29T11:35:28.685694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow tensorflow-hub --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/rnn'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone --depth 1 https://github.com/francisco-ortin/data-science-course.git\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "    !cp {directory}/img/* img/.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3e8a3ff34b7dc",
   "metadata": {
    "collapsed": false,
    "id": "e7c3e8a3ff34b7dc"
   },
   "source": [
    "## Parameters\n",
    "\n",
    " Now, we use the 100,000 most frequent words (5,000 in the previous example). We use 49,300 reviews for training (1,000 in the previous notebook), lazily loading the batches into memory (applying the ELMo embedding transformation). For test and validation, we keep using 350 reviews each."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Consider all the words with a frequency higher than this value. The higher, the more memory is needed.\n",
    "vocabulary_size = 100_000\n",
    "# Compute the maximum length of the reviews (for speeding up the training it is better to cut the reviews)\n",
    "max_review_length = 80\n",
    "# Max number of epochs to train the models (we use early stopping)\n",
    "n_epochs = 50\n",
    "# Embedding dimensions (ELMo embeddings have 1024 dimensions)\n",
    "embedding_dim = 1024\n",
    "# Number of sentences for validation and test, the remaining ones (49,000) will be used for training\n",
    "n_sentences_val, n_max_sentences_test = 350, 350"
   ],
   "metadata": {
    "id": "f5f7eb0c67d58ee5",
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:30.310771Z",
     "start_time": "2024-11-29T11:35:30.306174Z"
    }
   },
   "id": "f5f7eb0c67d58ee5",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset\n",
    "\n",
    "We load the dataset from the Keras API. 350 for testing, 350 for validation, and 49,300 for training."
   ],
   "metadata": {
    "collapsed": false,
    "id": "c50a52ab52105e74"
   },
   "id": "c50a52ab52105e74"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_half, y_half) = keras.datasets.imdb.load_data(num_words=vocabulary_size)\n",
    "assert n_sentences_val + n_max_sentences_test <= len(X_half), \"Not enough sentences for validation and test.\"\n",
    "X_test, X_val = X_half[-n_max_sentences_test:], X_half[-(n_sentences_val + n_max_sentences_test):-n_max_sentences_test]\n",
    "y_test, y_val = y_half[-n_max_sentences_test:], y_half[-(n_sentences_val + n_max_sentences_test):-n_max_sentences_test]\n",
    "# concat to X_train the remaining samples not used for validation and test\n",
    "X_train = np.concatenate((X_train, X_half[:-(n_sentences_val + n_max_sentences_test)]), axis=0)\n",
    "y_train = np.concatenate((y_train, y_half[:-(n_sentences_val + n_max_sentences_test)]), axis=0)\n",
    "X_half, y_half = None, None  # free memory\n",
    "\n",
    "print(f\"Training sequences: {len(X_train):,}.\\nValidation sequences: {len(X_val):,}.\\nTesting sequences: {len(X_test):,}.\")"
   ],
   "metadata": {
    "id": "73a4ffd175d95c8b",
    "outputId": "93005603-59dc-413b-b9ef-ff130250e411",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:31.187061Z",
     "start_time": "2024-11-29T11:35:30.313024Z"
    }
   },
   "id": "73a4ffd175d95c8b",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rest of the loading process is the same as in the previous notebook."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5f82fcb2a4d25d87"
   },
   "id": "5f82fcb2a4d25d87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's print some reviews. We need to convert the integers (token ids) back to words.\n",
    "word_to_index = {word: index+3 for word, index in keras.datasets.imdb.get_word_index().items()}  # word -> integer dictionary\n",
    "# The IMDB dataset reserves the 4 first indices for special tokens <PAD>, <START>, <OOV>, <END>\n",
    "index_to_word = {value: key for key, value in word_to_index.items()}  # integer -> word dictionary\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<START>\"\n",
    "index_to_word[2] = \"<OOV>\"\n",
    "index_to_word[3] = \"<END>\"\n",
    "\n",
    "\n",
    "def decode_review(encoded_review: list[int]) -> str:\n",
    "    \"\"\"Decode a review from a list of integers to a string.\"\"\"\n",
    "    return ' '.join(index_to_word.get(word_index, \"<OOV>\") for word_index in encoded_review)\n",
    "\n",
    "# We show the first reviews and their corresponding sentiment.\n",
    "print(\"First reviews in training set, with the corresponding labels:\")\n",
    "for (i, (review, label)) in enumerate(zip(X_train[:5], y_train[:5])):\n",
    "    print(f\"Review {i + 1}: {decode_review(review)}.\\nLabel: {label}.\")\n",
    "\n",
    "# We add padding to the reviews to have the same length. We use the `post` mode to pad and truncate at the end of the reviews.\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_review_length, padding=\"post\", truncating=\"post\")\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length, padding=\"post\", truncating=\"post\")"
   ],
   "metadata": {
    "id": "c98ebd5c105cfde3",
    "outputId": "3fab85ee-85c6-4920-aace-5a230b4812c7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:31.192043Z",
     "start_time": "2024-11-29T11:35:31.192043Z"
    }
   },
   "id": "c98ebd5c105cfde3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ELMo embeddings\n",
    "\n",
    "The `get_elmo_embeddings` function returns the ELMo embeddings for a list of sentences. It is used to convert the small test and validation sets to ELMo embeddings. For training, we have to do it lazily, since we cannot load all the data into memory at once."
   ],
   "metadata": {
    "collapsed": false,
    "id": "d564187c851e6ad8"
   },
   "id": "d564187c851e6ad8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "\n",
    "def get_elmo_embeddings(sentences: list[str]) -> np.array:\n",
    "    \"\"\"\n",
    "    Get ELMo embeddings for a list of sentences.\n",
    "    \"\"\"\n",
    "    # ELMo returns a tensor, but we want to extract the embeddings\n",
    "    embeddings = elmo.signatures['default'](tf.constant(sentences))['elmo']\n",
    "    return embeddings.numpy()  # Convert to numpy array for easier manipulation\n",
    "\n",
    "X_val_elmo = get_elmo_embeddings([decode_review(review) for review in X_val])\n",
    "X_test_elmo = get_elmo_embeddings([decode_review(review) for review in X_test])"
   ],
   "metadata": {
    "id": "43ef1964ad0dad2",
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:31.195207Z",
     "start_time": "2024-11-29T11:35:31.194204Z"
    }
   },
   "id": "43ef1964ad0dad2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lazy data generation for training\n",
    "\n",
    "The following `generate_data_lazy` function generates the training data in a lazy way, batch after batch, to avoid memory issues."
   ],
   "metadata": {
    "collapsed": false,
    "id": "be41a096e9dbefa9"
   },
   "id": "be41a096e9dbefa9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_data_lazy(X_train_p: np.array, y_train_p: np.array, batch_size: int, n_epochs_p:int) -> np.array:\n",
    "    \"\"\"\n",
    "    Generate training data in a lazy way, batch after batch, to avoid memory issues.\n",
    "    In this way, all the data is not loaded into memory at once.\n",
    "    :param X_train_p: The original training data with original shape (max_sentences_train, max_review_length)\n",
    "    :param y_train_p: The original training labels with original shape (max_sentences_train,)\n",
    "    :param batch_size: The batch size\n",
    "    :param n_epochs_p: The number of epochs\n",
    "    :return: Each batch of data, with ELMo embeddings of shape (batch_size, max_review_length, embedding_dim)\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epochs_p):\n",
    "        to_index = 0\n",
    "        for batch_number in range(X_train_p.shape[0] // batch_size):\n",
    "            from_index, to_index = batch_number*batch_size, (batch_number+1)*batch_size\n",
    "            X_train_elmo_batch = get_elmo_embeddings([decode_review(review) for review in X_train_p[from_index:to_index]])\n",
    "            yield X_train_elmo_batch, np.array(y_train_p[from_index:to_index])\n",
    "        X_train_elmo_batch = get_elmo_embeddings([decode_review(review) for review in X_train_p[to_index:]])\n",
    "        yield X_train_elmo_batch, np.array(y_train_p[to_index:])\n"
   ],
   "metadata": {
    "id": "419ca03571ca5b6d",
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:31.198752Z",
     "start_time": "2024-11-29T11:35:31.197678Z"
    }
   },
   "id": "419ca03571ca5b6d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the `train` function is modified to use the `generate_data_lazy` function to train the model using a lazy data generator. The rest of the parameters are the same as in the previous notebook."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2c21cb3cd6d19e51"
   },
   "id": "2c21cb3cd6d19e51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compile_train_evaluate(model_p: keras.Model, X_train_p: np.array, y_train_p: np.array,\n",
    "                           x_val_p: np.array, y_val_p: np.array, x_test_p: np.array, y_test_p: np.array,\n",
    "                           batch_size: int, epochs: int) -> (float, float, keras.Model):\n",
    "    \"\"\"\n",
    "    Compile, train and evaluate the model.\n",
    "    :param model_p: the model to compile, train and evaluate\n",
    "    :param X_train_p: train X sequences\n",
    "    :param y_train_p: train y labels\n",
    "    :param x_val_p: validation X sequences\n",
    "    :param y_val_p: validation y labels\n",
    "    :param x_test_p: test X sequences\n",
    "    :param y_test_p: test y labels\n",
    "    :param batch_size: batch size\n",
    "    :param epochs: number of epochs\n",
    "    :return: (test_loss, test_accuracy, model)\n",
    "    \"\"\"\n",
    "    # we compile and train the model\n",
    "    model_p.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "    model_p.fit(generate_data_lazy(X_train_p, y_train_p, batch_size, epochs),\n",
    "                batch_size=batch_size, epochs=epochs,\n",
    "                steps_per_epoch=X_train_p.shape[0] // batch_size + 1,\n",
    "                validation_data=(x_val_p, y_val_p),\n",
    "                callbacks=[early_stopping_callback])\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model_p.evaluate(x_test_p, y_test_p)\n",
    "    return loss, accuracy, model_p"
   ],
   "metadata": {
    "id": "a44817b16d6de233",
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:31.209781Z",
     "start_time": "2024-11-29T11:35:31.206531Z"
    }
   },
   "id": "a44817b16d6de233",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model compilation, training, and evaluation\n",
    "\n",
    "We create the same model as in the previous notebook, compile, train, and evaluate it."
   ],
   "metadata": {
    "collapsed": false,
    "id": "46fa539415841cc8"
   },
   "id": "46fa539415841cc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None, embedding_dim), dtype=\"float32\")\n",
    "# Add 2 LSTM layers\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2))(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Dropout layer with 20% rate\n",
    "x = layers.Dropout(0.2)(x)\n",
    "# Add a classifier (sigmoid activation function for binary classification)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "elmo_model = keras.Model(inputs, outputs)\n",
    "elmo_model.summary()\n",
    "\n",
    "# Compile, train and evaluate the model\n",
    "test_loss, test_accuracy, elmo_model = compile_train_evaluate(elmo_model, X_train, y_train, X_val_elmo, y_val, X_test_elmo, y_test, 32, n_epochs)\n",
    "print(f\"Test loss: {test_loss:.4f}.\\nTest accuracy: {test_accuracy:.4f}.\")"
   ],
   "metadata": {
    "id": "a10223fcc7637cb3",
    "outputId": "46e472f0-f32d-49f2-a63f-8c1c5c3f872d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:31.214295Z",
     "start_time": "2024-11-29T11:35:31.214295Z"
    }
   },
   "id": "a10223fcc7637cb3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Let's test the model with the same reviews as in the previous notebook. Feel free to add more reviews to test the model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "9e238be6c403d09f"
   },
   "id": "9e238be6c403d09f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "example_reviews = [\"The movie was a great waste of time. I is awful and boring.\",\n",
    "                   \"I loved the movie. The plot was amazing.\",\n",
    "                   \"This movie is not worth watching.\",\n",
    "                   \"Although the film is not a masterpiece, you may have a good time if your expectations are not high.\"]\n",
    "\n",
    "\n",
    "review_embeddings = get_elmo_embeddings(example_reviews)\n",
    "predictions = elmo_model.predict(review_embeddings)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Review {i + 1}: {example_reviews[i]}\")\n",
    "    print(f\"Probability of being positive: {prediction[0]:.4f}.\\n\")"
   ],
   "metadata": {
    "id": "9ef6db743cd17de",
    "outputId": "49d007e3-5a6d-499d-bb1f-400f0cb34f5a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T11:35:31.216530Z",
     "start_time": "2024-11-29T11:35:31.216530Z"
    }
   },
   "id": "9ef6db743cd17de",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions\n",
    "\n",
    "1. Has this model improved the accuracy of the similar model in the previous notebook?\n",
    "2. What is the main reason that explains the previous answer?\n",
    "3. When do you think you will need to train a model in this way?"
   ],
   "metadata": {
    "collapsed": false,
    "id": "9ef63507e7f6d1f1"
   },
   "id": "9ef63507e7f6d1f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "5461946251899efc"
   },
   "id": "5461946251899efc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
