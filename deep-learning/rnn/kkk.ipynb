{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/rnn/lstm_gru.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "87540b40e3958a06"
   },
   "id": "87540b40e3958a06"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f099c66705669f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:56:36.851184Z",
     "start_time": "2024-11-04T19:56:35.331390Z"
    },
    "id": "5f099c66705669f8"
   },
   "outputs": [],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/rnn'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "    !cp {directory}/img/* img/.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "# Simulated example corpus (adjust this with your own dataset)\n",
    "with open('data/names.txt', 'r') as file:\n",
    "    train_text = file.read().lower()\n",
    "\n",
    "\n",
    "# Define the padding character and the max sequence length\n",
    "padding_char = '\\x00'  # Padding character (you can use any non-existing char)\n",
    "max_seq_length = 20  # Set your max sequence length\n",
    "\n",
    "# Create vocabulary of unique characters (including the padding character)\n",
    "vocabulary_chars = sorted(list(set(train_text + padding_char)))\n",
    "print(\"Total unique chars:\", len(vocabulary_chars))\n",
    "char_to_index = {char: index for index, char in enumerate(vocabulary_chars)}\n",
    "index_to_char = {index: char for index, char in enumerate(vocabulary_chars)}\n",
    "\n",
    "\n",
    "# cut the text in sequences of max_seq_length characters\n",
    "lines = [line + \"\\n\" for line in train_text.splitlines()]\n",
    "random.shuffle(lines)\n",
    "input_sequences = []\n",
    "output_chars = []\n",
    "for line in lines:\n",
    "    for char_index_in_line in range(1, len(line)):\n",
    "        # Pad input sequence to max_seq_length\n",
    "        input_sequence = line[:char_index_in_line]\n",
    "        padded_input_sequence = input_sequence .ljust(max_seq_length, padding_char)\n",
    "        input_sequences.append(padded_input_sequence)\n",
    "        output_char = line[char_index_in_line]\n",
    "        output_chars.append(output_char)\n",
    "        if output_char == '\\n':\n",
    "            break\n",
    "\n",
    "print(f\"Number of input sequences: {len(input_sequences):,}.\")\n",
    "print(f\"Number of output chars: {len(output_chars):,}.\")\n",
    "\n",
    "print(\"Input sequences: \", input_sequences[:15])\n",
    "print(\"Output chars: \", output_chars[:15])\n",
    "\n",
    "\n",
    "# One-hot encode the input sequences and output characters\n",
    "X_train_ds = np.zeros((len(input_sequences), max_seq_length, len(vocabulary_chars)), dtype=\"float\")\n",
    "y_train_ds = np.zeros((len(input_sequences), len(vocabulary_chars)), dtype=\"float\")\n",
    "\n",
    "for sequence_index, input_sequence in enumerate(input_sequences):\n",
    "    for char_index, input_char in enumerate(input_sequence):\n",
    "        X_train_ds[sequence_index, char_index, char_to_index[input_char]] = 1\n",
    "    output_char = output_chars[sequence_index]\n",
    "    y_train_ds[sequence_index, char_to_index[output_char]] = 1\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    # Masking layer: ignores the padding value\n",
    "    layers.Masking(mask_value=char_to_index[padding_char], input_shape=(None, len(vocabulary_chars))),\n",
    "    #layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(128),\n",
    "    layers.Dense(len(vocabulary_chars), activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "epochs = 2  # For demonstration, use more epochs for real training\n",
    "batch_size = 32\n",
    "model.fit(X_train_ds, y_train_ds, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "\n",
    "# Sampling function (to generate text based on model predictions)\n",
    "def sample(predictions: np.array, temperature: float = 1.0) -> int:\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "# Inference with variable input length (pad input dynamically)\n",
    "start_index = random.randint(0, len(train_text) - max_seq_length - 1)\n",
    "#input_sequence = train_text[start_index:start_index + 25]  # Example input of variable length (25 chars)\n",
    "original_input_sequence = padding_char\n",
    "\n",
    "\n",
    "for temperature in [0.5, 0.8, 1.0, 1.2]:\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    input_sequence = original_input_sequence\n",
    "    generated = original_input_sequence\n",
    "    next_char = ''\n",
    "    while next_char != '\\n' and next_char != padding_char:\n",
    "        #padded_input_sequence = input_sequence.ljust(max_seq_length, padding_char)\n",
    "        #print(\"Input sequence: \", input_sequence)\n",
    "        #print(\"Padded input sequence: \", padded_input_sequence)\n",
    "        X_to_predict = np.zeros((1, len(input_sequence), len(vocabulary_chars)))\n",
    "        for char_index, input_char in enumerate(input_sequence):\n",
    "            X_to_predict[0, char_index, char_to_index[input_char]] = 1.0\n",
    "\n",
    "        preds = model.predict(X_to_predict, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "        input_sequence = (input_sequence + next_char)[-max_seq_length:]  # get the last max_seq_length characters\n",
    "        input_sequence = input_sequence\n",
    "        generated += next_char\n",
    "    print(\"Generated: \", generated)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e9ab42d81e066fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
