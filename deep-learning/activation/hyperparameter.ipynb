{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/deep-learning/activation/hyperparameter.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ca88ed918d07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter tuning/optimization\n",
    "\n",
    "When defining ANNs, there are many hyperparameters to tweak. Not only ANN architecture parameters (the number of layers, the number of neurons and the type of activation function to use in each layer) but also the way they are trained (the initialization logic, the type of optimizer to use, its learning rate, the batch size, and more).\n",
    "\n",
    "The hyperparameter tuning/optimization problem involves finding the best set of hyperparameters for a machine learning model to optimize its performance on a given task or dataset. The goal of hyperparameter tuning/optimization is to search the hyperparameter space efficiently to find the set of hyperparameters that maximizes the model's performance metric, such as accuracy, loss, or some other evaluation metric. This process typically involves conducting multiple experiments with different hyperparameter configurations, training and evaluating the model for each configuration, and selecting the configuration that yields the best performance.\n",
    "\n",
    "As mentioned in this course, we should not use the test set to tune the hyperparameters. Instead, we should split the training set into a training set and a validation set. The training set is used to train the model, while the validation set is used to evaluate the model's performance on unseen data and tune the hyperparameters. Once the hyperparameters are tuned and the model is trained, we can evaluate the model's performance on the test set to get an unbiased estimate of its performance on unseen data.\n",
    "\n",
    "There are many different approaches to perform hyperparameter tuning/optimization:\n",
    "- Manual search: manually selecting hyperparameters based on intuition, experience, or trial and error.\n",
    "- Grid search: exhaustively searching the hyperparameter space by evaluating all possible combinations of hyperparameters.\n",
    "- Random search: randomly sampling hyperparameters from a predefined search space and evaluating them.\n",
    "- Bayesian optimization: using probabilistic models to model the hyperparameter space and guide the search process.\n",
    "- Evolutionary algorithms: using evolutionary algorithms to evolve a population of hyperparameter configurations over time.\n",
    "\n",
    "The three first methods are very easy to implement but are not very efficient. In this notebook we will use [Bayesian optimization](https://en.wikipedia.org/wiki/Bayesian_optimization). We will use the `Keras Tuner` library, which provides a simple and efficient way to perform hyperparameter tuning/optimization in Keras models.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72ea15f3d70547f7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn keras tensorflow keras_tuner --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/deep-learning/activation'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone --depth 1 https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/img/* img/.\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T16:16:53.579517900Z",
     "start_time": "2024-10-08T16:16:51.973343Z"
    }
   },
   "id": "5f099c66705669f8",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "\n",
    "We use the [Fashion-MNIST](https://keras.io/api/datasets/fashion_mnist/) dataset. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7c3e8a3ff34b7dc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train = (1000, 28, 28) and y_train = (1000,).\n",
      "Shape of X_val = (1000, 28, 28) and y_val = (1000,).\n",
      "Shape of X_test = (10000, 28, 28) and y_test = (10000,).\n"
     ]
    }
   ],
   "source": [
    "# We download fashion MNIST the dataset from keras.\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "# we split the training set into training and validation sets\n",
    "N_TRAIN_INSTANCES, N_VAL_INSTANCES = 1_000, 1_000\n",
    "X_train, y_train = X_train_full[:N_TRAIN_INSTANCES], y_train_full[:N_TRAIN_INSTANCES]\n",
    "X_val, y_val = X_train_full[-N_VAL_INSTANCES:], y_train_full[-N_VAL_INSTANCES:]\n",
    "CLASS_LABELS = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "# Show shapes of all the datasets\n",
    "print(f\"Shape of X_train = {X_train.shape} and y_train = {y_train.shape}.\")\n",
    "print(f\"Shape of X_val = {X_val.shape} and y_val = {y_val.shape}.\")\n",
    "print(f\"Shape of X_test = {X_test.shape} and y_test = {y_test.shape}.\")\n",
    "# We rescale the colors to real numbers between 0 and 1\n",
    "X_train, X_val, X_test = X_train / 255, X_val / 255, X_test / 255"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T16:12:53.091979400Z",
     "start_time": "2024-10-08T16:12:52.734591Z"
    }
   },
   "id": "33531d19a75de77f",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter variable specification\n",
    "\n",
    "To use the `Keras Turner` library, we define a subclass of `kt.HyperModel` that defines the model architecture and hyperparameters to tune. For example, we could tune the number of hidden layers, the number of neurons in each hidden layer, the learning rate, and the optimizer. This is done in the `build` method of the `HyperModel` subclass. The `fit` method is used to train the model with a given set of hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "802ac98cb6d019ad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MyClassificationHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        # Searches for different number of hidden layers between 0 and 8\n",
    "        n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8)  # number of hidden layers\n",
    "        n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)  # number of neurons per layer\n",
    "        learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")  # learning rate\n",
    "        optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])  # optimizer\n",
    "        if optimizer == \"sgd\":\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        # model creation\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        for _ in range(n_hidden):\n",
    "            model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dense(len(CLASS_LABELS), activation=\"softmax\"))\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, X, y, **kwargs):\n",
    "        return model.fit(X, y, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T16:12:55.236573700Z",
     "start_time": "2024-10-08T16:12:55.230983600Z"
    }
   },
   "id": "ba1108741a17ef06",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter search\n",
    "\n",
    "`Keras Turner` provides different ways to search for hyperparameters: `RandomSearch`, `BayesianOptimization`, `Hyperband` (early stopping method to prune poor configurations), `Greedy` (a simple search algorithm that greedily selects the best configuration at each step), and `Sklearn` (a search algorithm that uses scikit-learn's search methods). In this example, we use `BayesianOptimization`. It performs a probabilistic search (a Gaussian process) that approximates the objective function (model performance metric) based on the observed evaluations of hyperparameters. This allows it to explore the hyperparameter space more efficiently and find better configurations in fewer iterations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52235b5e38e3ccf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 02s]\n",
      "val_accuracy: 0.7590000033378601\n",
      "\n",
      "Best val_accuracy So Far: 0.7879999876022339\n",
      "Total elapsed time: 00h 00m 40s\n"
     ]
    }
   ],
   "source": [
    "# first, we define the search method\n",
    "bayesian_opt_tuner = kt.BayesianOptimization(\n",
    "    MyClassificationHyperModel(),\n",
    "    objective=\"val_accuracy\",  # objective function of the optimization problem\n",
    "    max_trials=10,  # max number of executions\n",
    "    directory=\"hyperparams\",  # output folder\n",
    "    overwrite=False)  # does not delete the existing models in the output folder upon new execution\n",
    "# second, we perform the search\n",
    "bayesian_opt_tuner.search(X_train, y_train, epochs=10,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          callbacks=[tf.keras.callbacks.EarlyStopping()])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0e285a33f74265b",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best hyperparameter, performance and mode retrieval\n",
    "\n",
    "We can retrieve the best hyperparameters, performance metrics, and model from the tuner."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edfd1a4a83763498"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_hidden': 3, 'n_neurons': 198, 'learning_rate': 0.0002990356912538018, 'optimizer': 'adam'}.\n",
      "Validation accuracy with the best hyperparameters: 0.7880.\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.7810\n",
      "Best model's loss (test set): 0.6144. Best model's accuracy (test set): 0.7810.\n"
     ]
    }
   ],
   "source": [
    "# get the first hyperparameter set in descending order of performance (the best ones)\n",
    "hyperparameters = bayesian_opt_tuner.get_best_hyperparameters()[0]   \n",
    "print(f\"Best hyperparameters: {hyperparameters.values}.\")\n",
    "\n",
    "# get the best trial (the one with the best performance); then ge the validation accuracy\n",
    "best_trial = bayesian_opt_tuner.oracle.get_best_trials()[0]\n",
    "val_accuracy = best_trial.metrics.get_last_value(\"val_accuracy\")\n",
    "print(f\"Validation accuracy with the best hyperparameters: {val_accuracy:.4f}.\")\n",
    "\n",
    "# get the best model and evaluate it on the test set\n",
    "best_model = bayesian_opt_tuner.get_best_models()[0]\n",
    "evaluation_results = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Best model's loss (test set): {evaluation_results[0]:.4f}. Best model's accuracy (test set): {evaluation_results[1]:.4f}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T16:16:27.134463600Z",
     "start_time": "2024-10-08T16:16:25.947740200Z"
    }
   },
   "id": "e59cf1f79ad18fa0",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨ \n",
    "\n",
    "1. Do you think the search will find a better model if it keeps searching?\n",
    "2. Considering the hyperparameter tuning process as a black box, what sets do we have to pass it (choose one answer):\n",
    "a) train.\n",
    "b) train and validation.\n",
    "c) train, validation and test.\n",
    "3. Why?\n",
    "4. Do you take the best model out of the hyperparameter tuning and use it for inference?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6440c3da7d486fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "161159eac285bee5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
