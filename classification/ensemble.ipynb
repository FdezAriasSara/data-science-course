{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/francisco-ortin/data-science-course/blob/main/classification/ensemble.ipynb)\n",
    "[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77f76148d5f67905"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble methods\n",
    "\n",
    "This notebook tackles two important things: ensemble methods and how to compare the performance of different models. Ensemble methods are a type of machine learning technique that combines several models to improve the performance of the model. To compare the performance of different models, we have to used statistical tests, since the models are trained with stochastic procedures.\n",
    "\n",
    "We use the [Titanic Disaster dataset](https://www.kaggle.com/c/titanic/data?select=test.csv) stored in `data/titanic.csv`. The dataset has de following features:\n",
    "- PassengerId: unique identifier for each passenger.\n",
    "- Survived: target variable (0 = No, 1 = Yes).\n",
    "- Pclass: ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).\n",
    "- Name: name of the passenger.\n",
    "- Sex: \"male\" or \"female\".\n",
    "- Age: age in years.\n",
    "- SibSp: number of siblings/spouses aboard.\n",
    "- Parch: number of parents/children aboard.\n",
    "- Ticket: ticket number.\n",
    "- Fare: passenger fare.\n",
    "- Cabin: cabin number.\n",
    "- Embarked: port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "137bff049a6ed8fa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure the required packages are installed\n",
    "%pip install pandas seaborn matplotlib scikit-learn xgboost --quiet\n",
    "# if running in colab, install the required packages and copy the necessary files\n",
    "directory='data-science-course/classification'\n",
    "if get_ipython().__class__.__module__.startswith('google.colab'):\n",
    "    !git clone https://github.com/francisco-ortin/data-science-course.git  2>/dev/null\n",
    "    !cp --update {directory}/*.py .\n",
    "    !mkdir -p img data\n",
    "    !cp {directory}/data/* data/.\n",
    "\n",
    "# import the required modules\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from time import time\n",
    "\n",
    "import utils\n",
    "\n",
    "random_state = 42"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T16:16:30.097467700Z",
     "start_time": "2024-10-05T16:16:28.362674700Z"
    }
   },
   "id": "abd29d079dd54ab4",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "Let's load and clean the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3fc427e49037f9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_file_name = 'data/titanic.csv'\n",
    "independent_vars = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "dependent_var = 'Survived'\n",
    "class_names = ['Not Survived', 'Survived']\n",
    "# load anc clean the dataset\n",
    "dataset, independent_vars = utils.load_clean_titanic_dataset(dataset_file_name, independent_vars, dependent_var)\n",
    "# Split the dataset into training and testing sets\n",
    "(X_train, y_train), (X_test, y_test) = utils.split_dataset(dataset, independent_vars, dependent_var,\n",
    "                            0.6, random_state)  # we choose a train size = 0.4 test size of 0.6 on purpose to have higher variance"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T16:16:30.163420200Z",
     "start_time": "2024-10-05T16:16:30.100127300Z"
    }
   },
   "id": "52b274772e8eefe0",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Different binary classification models\n",
    "\n",
    "We create different binary classification models with the default hyperparameters. Please, note that we are not tuning the hyperparameters of the models. This is important, because the performance of the models could be improved by tuning the hyperparameters. Particularly, the Random Forest and XGBoost models have many hyperparameters that could be tuned and improved."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2e1b361dc4ea4dd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Single-value evaluation method -----\n",
      "Model: DecisionTreeClassifier.\n",
      "\tAccuracy: 0.7495.\n",
      "\tF1 Score: 0.6854.\n",
      "Model: RandomForestClassifier.\n",
      "\tAccuracy: 0.7664.\n",
      "\tF1 Score: 0.7126.\n",
      "Model: XGBClassifier.\n",
      "\tAccuracy: 0.7757.\n",
      "\tF1 Score: 0.7260.\n",
      "Time elapsed: 0.29 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'-'*5} Single-value evaluation method {'-'*5}\")\n",
    "time_before = time()\n",
    "# Let's different DT-based models\n",
    "dt_model = DecisionTreeClassifier(random_state=random_state)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=random_state)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=random_state)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "utils.evaluate_models([dt_model, rf_model, xgb_model], X_test, y_test)\n",
    "print(f\"Time elapsed: {time() - time_before:.2f} seconds.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T16:16:30.425803700Z",
     "start_time": "2024-10-05T16:16:30.124901800Z"
    }
   },
   "id": "b767b882934a295b",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨\n",
    "\n",
    "1. Can we state that XGBoost is the best model?\n",
    "2. Set the random_state to None and run the code many times. Is XGBoost always the best model? What is happening?\n",
    "3. What could we do to improve the comparison?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e3d3bd899b07df7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87156cc804b49eb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model comparison\n",
    "\n",
    "### Method 1: Confidence intervals\n",
    "\n",
    "The first method we are using to compare models train and evaluate the models N times and compute the 95% confidence intervals. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb5dafad8526ac8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Re-train and re-evaluate method -----\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the models n_times and store the evaluation results in accuracies and f1_scores\n",
    "print(f\"\\n{'-'*5} Re-train and re-evaluate method {'-'*5}\")\n",
    "time_before = time()\n",
    "n_times = 30\n",
    "accuracies = dict()\n",
    "f1_scores = dict()\n",
    "for _ in range(n_times):\n",
    "    (temp_X_train, temp_y_train), (temp_X_test, temp_y_test) = utils.split_dataset(dataset, independent_vars, dependent_var,\n",
    "                                                               0.6, random_state=None)\n",
    "    dt_model = DecisionTreeClassifier(random_state=None)\n",
    "    dt_model.fit(temp_X_train, temp_y_train)\n",
    "    rf_model = RandomForestClassifier(random_state=None)\n",
    "    rf_model.fit(temp_X_train, temp_y_train)\n",
    "    xgb_model = xgb.XGBClassifier(random_state=None)\n",
    "    xgb_model.fit(temp_X_train, temp_y_train)\n",
    "    models = [dt_model, rf_model, xgb_model]\n",
    "    metrics = utils.evaluate_models(models, temp_X_test, temp_y_test, verbose=False)\n",
    "    for model_name, (accuracy, f1_score_value) in zip(models, metrics):\n",
    "        accuracies.setdefault(model_name.__class__.__name__, []).append(accuracy)\n",
    "        f1_scores.setdefault(model_name.__class__.__name__, []).append(f1_score_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T16:16:37.853743Z",
     "start_time": "2024-10-05T16:16:30.422673700Z"
    }
   },
   "id": "3ef4181303548aa6",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now compute and visualize the confidence intervals of each model and metric."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "316f70b41ff339c2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTreeClassifier.\n",
      "\tAccuracy mean: 0.7680. CI: (0.7591361987943405, 0.776938567560799).\n",
      "\tF1 Score: 0.6821. CI: (0.6683912694960683, 0.6957462759811113).\n",
      "Model: RandomForestClassifier.\n",
      "\tAccuracy mean: 0.7720. CI: (0.7642338617189826, 0.7796913719258778).\n",
      "\tF1 Score: 0.6929. CI: (0.6825579938557793, 0.7031504764987067).\n",
      "Model: XGBClassifier.\n",
      "\tAccuracy mean: 0.7829. CI: (0.7763415583106815, 0.7893905289167328).\n",
      "\tF1 Score: 0.7062. CI: (0.6975814564985595, 0.714759473542269).\n",
      "Time elapsed: 7.43 seconds.\n"
     ]
    }
   ],
   "source": [
    "for model_name in accuracies:\n",
    "    accuracy_mean, accuracy_confidence_interval = utils.confidence_interval(accuracies[model_name], 0.95)\n",
    "    f1_score_mean, f1_score_confidence_interval = utils.confidence_interval(f1_scores[model_name], 0.95)\n",
    "    print(f\"Model: {model_name}.\")\n",
    "    print(f\"\\tAccuracy mean: {accuracy_mean:.4f}. CI: {accuracy_confidence_interval}.\")\n",
    "    print(f\"\\tF1 Score: {f1_score_mean:.4f}. CI: {f1_score_confidence_interval}.\")\n",
    "\n",
    "print(f\"Time elapsed: {time() - time_before:.2f} seconds.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T16:16:37.865965700Z",
     "start_time": "2024-10-05T16:16:37.853743Z"
    }
   },
   "id": "e1b04729b6fb43fa",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨\n",
    "\n",
    "4. Are there differences with the previous values?\n",
    "5. Why?\n",
    "6. What method do you think is better?\n",
    "7. Is there any modification of the best, average, and worst models?\n",
    "8. Are there significant differences between the models?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b4c7ae220e5ad64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d7fc7b202ba7f59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Method 2: Bootstrap\n",
    "\n",
    "We do not retrain the model. Instead, we use bootstrap on the test set to estimate the confidence intervals.\n",
    "\n",
    "*Notice*. This method is used when retraining the models is so expensive that we cannot afford to retrain them N times. A common example is a deep learning model that takes hours to train. In this case, we use the test set to compute the confidence intervals of the metrics using a method called bootstrapping. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b9ba27268be4957"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Bootstrapping method -----\n",
      "Model: DecisionTreeClassifier.\n",
      "\tAccuracy mean: 0.7495. CI: (0.7483864180492062, 0.7506528342872422).\n",
      "\tF1 Score: 0.6853. CI: (0.6838199170049631, 0.6868258975189289).\n",
      "Model: RandomForestClassifier.\n",
      "\tAccuracy mean: 0.7655. CI: (0.764376426259316, 0.7665880597219923).\n",
      "\tF1 Score: 0.7114. CI: (0.7099940157216071, 0.7129007509626097).\n",
      "Model: XGBClassifier.\n",
      "\tAccuracy mean: 0.7753. CI: (0.7742390269187925, 0.7764376085952261).\n",
      "\tF1 Score: 0.7255. CI: (0.7240700411891269, 0.7269247001146079).\n",
      "Time elapsed: 25.79 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'-'*5} Bootstrapping method {'-'*5}\")\n",
    "time_before = time()\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=random_state)\n",
    "dt_model.fit(X_train, y_train)\n",
    "rf_model = RandomForestClassifier(random_state=random_state)\n",
    "rf_model.fit(X_train, y_train)\n",
    "xgb_model = xgb.XGBClassifier(random_state=random_state)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "models = [dt_model, rf_model, xgb_model]\n",
    "\n",
    "# Perform bootstrapping\n",
    "n_bootstrap_samples = 1000\n",
    "bootstrap_accuracies = dict()\n",
    "bootstrap_f1_scores = dict()\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    # Resample with replacement, both X_test and the corresponding y_test at the same time (important)\n",
    "    resampled_X_test, resampled_y_test = resample(X_test, y_test, replace=True)\n",
    "    # Calculate the metrics for each model and store it in the two dictionaries\n",
    "    for model in models:\n",
    "        resampled_y_pred = model.predict(resampled_X_test)\n",
    "        accuracy = accuracy_score(resampled_y_test, resampled_y_pred)\n",
    "        f1_score_value = f1_score(resampled_y_test, resampled_y_pred)\n",
    "        bootstrap_accuracies.setdefault(model.__class__.__name__, []).append(accuracy)\n",
    "        bootstrap_f1_scores.setdefault(model.__class__.__name__, []).append(f1_score_value)\n",
    "\n",
    "# Compute and visualize the confidence intervals of each model and metric\n",
    "for model_name in bootstrap_accuracies:\n",
    "    accuracy_mean, accuracy_confidence_interval = utils.confidence_interval(bootstrap_accuracies[model_name], 0.95)\n",
    "    f1_score_mean, f1_score_confidence_interval = utils.confidence_interval(bootstrap_f1_scores[model_name], 0.95)\n",
    "    print(f\"Model: {model_name}.\")\n",
    "    print(f\"\\tAccuracy mean: {accuracy_mean:.4f}. CI: {accuracy_confidence_interval}.\")\n",
    "    print(f\"\\tF1 Score: {f1_score_mean:.4f}. CI: {f1_score_confidence_interval}.\")\n",
    "\n",
    "print(f\"Time elapsed: {time() - time_before:.2f} seconds.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T16:17:03.691901100Z",
     "start_time": "2024-10-05T16:16:37.863796700Z"
    }
   },
   "id": "daddbc5dc30710e9",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ✨ Questions ✨\n",
    "\n",
    "9. Are there differences with the previous values? Why?\n",
    "10. What method takes longer? Why?\n",
    "11. In deep learning scenarios, do you think there will be similar execution-time differences?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f03168a7c462169f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Answers\n",
    "\n",
    "*Write your answers here.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b957c4e24f6092ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
